{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5RayCvi9drR"
      },
      "source": [
        "# Deliverable 4\n",
        "----\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqvVtBXB9NP5"
      },
      "source": [
        "# 1. Import and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd6H8a2p6byY",
        "outputId": "7aa43742-99f3-401d-d2d0-c94f0b3b5eed"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dv63B9o986kt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HOME is /Users/danyaalsadiq/Downloads/DL_Group_5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(f'HOME is {HOME}')\n",
        "import sys\n",
        "import torch\n",
        "# !rm -rf /content/GroundingDINO\n",
        "# !rm -rf /content/requirements.txt\n",
        "# !rm -rf /content/groundingdino_swint_ogc.pth\n",
        "# !rm -rf /content/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jj1P0qFw5o3w"
      },
      "outputs": [],
      "source": [
        "\n",
        "# %cd {HOME}\n",
        "# !git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "# %cd './GroundingDINO'\n",
        "# !pip install -r requirements.txt\n",
        "# print(f'HOME is {HOME}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x8_Ca9M60tj",
        "outputId": "60a1426b-3297-42d6-c35f-1a9ad402ce07"
      },
      "outputs": [],
      "source": [
        "# cuda_file = f\"/{HOME}/GroundingDINO/groundingdino/models/GroundingDINO/csrc/MsDeformAttn/ms_deform_attn_cuda.cu\"\n",
        "\n",
        "# # Replace deprecated value.type() usage\n",
        "# !sed -i '' 's/value\\.type()/value.scalar_type()/g' \"$cuda_file\"\n",
        "# !sed -i '' 's/::detail::scalar_type(the_type)/the_type/g' \"$cuda_file\"\n",
        "# !sed -i '' 's/value\\.scalar_type()\\.is_cuda()/value.is_cuda()/g' \"$cuda_file\"\n",
        "\n",
        "# print(\"Fully patched CUDA file for modern PyTorch compatibility\")\n",
        "\n",
        "\n",
        "# # === PATCH setup.py to return [] instead of None in get_extensions() ===\n",
        "# setup_file = f\"/{HOME}/GroundingDINO/setup.py\"\n",
        "\n",
        "# with open(setup_file, \"r\") as f:\n",
        "#     lines = f.readlines()\n",
        "\n",
        "# patched_lines = []\n",
        "# inside_else_block = False\n",
        "\n",
        "# for line in lines:\n",
        "#     # Detect the \"else:\" block under the CUDA check\n",
        "#     if line.strip() == \"else:\" and \"CUDA\" in \"\".join(patched_lines[-2:]):\n",
        "#         inside_else_block = True\n",
        "#         patched_lines.append(line)\n",
        "#         continue\n",
        "\n",
        "#     if inside_else_block:\n",
        "#         if \"return None\" in line:\n",
        "#             line = line.replace(\"return None\", \"return []  # patched for Colab\")\n",
        "#             inside_else_block = False  # Only patch once\n",
        "#     patched_lines.append(line)\n",
        "\n",
        "# with open(setup_file, \"w\") as f:\n",
        "#     f.writelines(patched_lines)\n",
        "\n",
        "# print(\"Patched setup.py to return [] instead of None (avoids setup crash)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5d_bdhDyQ8jr"
      },
      "outputs": [],
      "source": [
        "# # ─── MICRO-PATCH GroundingDINO/bertwarper.py ──────────────────────────\n",
        "# bert_file = f\"/{HOME}/GroundingDINO/groundingdino/models/GroundingDINO/bertwarper.py\"\n",
        "\n",
        "# import re, pathlib, textwrap\n",
        "\n",
        "# lines   = pathlib.Path(bert_file).read_text().splitlines(keepends=True)\n",
        "# output  = []\n",
        "# patched = False\n",
        "\n",
        "# pat = re.compile(r\"^\\s*cate_to_token_mask_list\\s*=\\s*\\[\\s*torch\\.stack\")\n",
        "\n",
        "# for i, line in enumerate(lines):\n",
        "\n",
        "#     # ── find the original one-liner list-comprehension ────────────────\n",
        "#     if not patched and pat.search(line):\n",
        "#         indent = re.match(r\"^\\s*\", line).group(0)          # keep original indent\n",
        "\n",
        "#         patch = textwrap.indent(textwrap.dedent(f\"\"\"\n",
        "#             # ===== PATCHED: avoid empty TensorList crash =====\n",
        "#             max_seq_len = input_ids.shape[1]\n",
        "#             device      = input_ids.device\n",
        "#             safe_cate_to_token_mask_list = []\n",
        "#             for _mask_list in cate_to_token_mask_list:\n",
        "#                 if len(_mask_list) == 0:\n",
        "#                     _mask_list = [torch.zeros((1, max_seq_len),\n",
        "#                                              dtype=torch.bool,\n",
        "#                                              device=device)]\n",
        "#                 safe_cate_to_token_mask_list.append(\n",
        "#                     torch.stack(_mask_list, dim=0)\n",
        "#                 )\n",
        "#             cate_to_token_mask_list = safe_cate_to_token_mask_list\n",
        "#             # ===== END PATCH ===========================================\n",
        "#         \"\"\"), indent)\n",
        "\n",
        "#         output.append(patch)\n",
        "#         patched = True\n",
        "#         continue                      # ← skip the original risky line\n",
        "\n",
        "#     # skip the second line of the old list-comprehension\n",
        "#     if patched and \"for cate_to_token_mask_list\" in line:\n",
        "#         continue\n",
        "\n",
        "#     output.append(line)\n",
        "\n",
        "# # write back\n",
        "# pathlib.Path(bert_file).write_text(\"\".join(output))\n",
        "# print(\"bertwarper.py patched – empty-TensorList crash eliminated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7wtii3pK6-pd"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -e . -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0NlacTF7PZb",
        "outputId": "b4912345-65cb-426f-eef5-b102a30b17e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/danyaalsadiq/Downloads/DL_Group_5\n",
            "HOME IS /Users/danyaalsadiq/Downloads/DL_Group_5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/danyaalsadiq/Library/Python/3.12/lib/python/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "print(f'HOME IS {HOME}')\n",
        "# !wget -q -P weights https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "\n",
        "# !curl -L -o weights/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GxzoAx69QuZ",
        "outputId": "039e3ce4-a84b-4afc-8b1f-b82ad7794fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/danyaalsadiq/Downloads/DL_Group_5/GroundingDINO\n",
            "GroundingDINO modules imported successfully!\n",
            "/Users/danyaalsadiq/Downloads/DL_Group_5\n",
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from timm import create_model\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.patches as patches\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Import GroundingDINO modules\n",
        "%cd {HOME}/GroundingDINO\n",
        "\n",
        "try:\n",
        "    from groundingdino.models import build_model\n",
        "    from groundingdino.util.slconfig import SLConfig\n",
        "    from groundingdino.util.utils import clean_state_dict\n",
        "    from groundingdino.util.inference import load_image, load_model, get_phrases_from_posmap\n",
        "    print(\"GroundingDINO modules imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing GroundingDINO modules: {e}\")\n",
        "    print(\"Please check your installation and try again.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "%cd {HOME}\n",
        "\n",
        "# Check if imports were successful\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TToUzhQL9QQM"
      },
      "source": [
        "# 2. RSVG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7SFFTJnFCf4x"
      },
      "outputs": [],
      "source": [
        "class VisualEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisualEncoder, self).__init__()\n",
        "        self.backbone = create_model('resnet50', pretrained=True, features_only=True)\n",
        "        self.conv6_1 = nn.Conv2d(2048, 128, kernel_size=1, stride=1)\n",
        "        self.conv6_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv7_1 = nn.Conv2d(256, 128, kernel_size=1, stride=1)\n",
        "        self.conv7_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv8_1 = nn.Conv2d(256, 128, kernel_size=1, stride=1)\n",
        "        self.conv8_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        f1 = features[-1]\n",
        "        x = self.relu(self.conv6_1(f1))\n",
        "        f2 = self.relu(self.conv6_2(x))\n",
        "        x = self.relu(self.conv7_1(f2))\n",
        "        f3 = self.relu(self.conv7_2(x))\n",
        "        x = self.relu(self.conv8_1(f3))\n",
        "        f4 = self.relu(self.conv8_2(x))\n",
        "        return [f1, f2, f3, f4]\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def forward(self, texts):\n",
        "        if isinstance(texts, list) and isinstance(texts[0], str):\n",
        "            tokens = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=40)\n",
        "        else:\n",
        "            # Handle different text formats\n",
        "            if isinstance(texts, list) and isinstance(texts[0], dict) and \"caption\" in texts[0]:\n",
        "                text_strs = [text[\"caption\"] for text in texts]\n",
        "            else:\n",
        "                text_strs = [str(text) for text in texts]\n",
        "            tokens = self.tokenizer(text_strs, return_tensors='pt', padding=True, truncation=True, max_length=40)\n",
        "\n",
        "        device = next(self.bert.parameters()).device\n",
        "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "        outputs = self.bert(**tokens)\n",
        "        word_embeddings = outputs.last_hidden_state\n",
        "        sentence_embedding = outputs.pooler_output.unsqueeze(1)\n",
        "        return [word_embeddings, sentence_embedding]\n",
        "\n",
        "class MLCM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLCM, self).__init__()\n",
        "        self.linear_v1 = nn.Linear(2048, 256)\n",
        "        self.linear_v2 = nn.Linear(256, 256)\n",
        "        self.linear_v3 = nn.Linear(256, 256)\n",
        "        self.linear_v4 = nn.Linear(256, 256)\n",
        "        self.linear_t_word = nn.Linear(768, 256)\n",
        "        self.linear_t_sent = nn.Linear(768, 256)\n",
        "        self.L = 6\n",
        "        self.N = 6\n",
        "        self.cross_attn_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(256, 8, dropout=0.1) for _ in range(self.L)\n",
        "        ])\n",
        "        self.cross_attn_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(256) for _ in range(self.L)\n",
        "        ])\n",
        "        self.self_attn_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(256, 8, dropout=0.1) for _ in range(self.N)\n",
        "        ])\n",
        "        self.self_attn_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(256) for _ in range(self.N)\n",
        "        ])\n",
        "        self.ffn_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(256, 2048),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(2048, 256)\n",
        "            ) for _ in range(self.L + self.N)\n",
        "        ])\n",
        "        self.ffn_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(256) for _ in range(self.L + self.N)\n",
        "        ])\n",
        "\n",
        "    def forward(self, visual_features, text_features):\n",
        "        f1, f2, f3, f4 = visual_features\n",
        "        B, C, H, W = f1.shape\n",
        "        f1_flat = f1.reshape(B, C, H*W).permute(0, 2, 1)\n",
        "        f1_proj = self.linear_v1(f1_flat)\n",
        "        B, C, H, W = f2.shape\n",
        "        f2_flat = f2.reshape(B, C, H*W).permute(0, 2, 1)\n",
        "        f2_proj = self.linear_v2(f2_flat)\n",
        "        B, C, H, W = f3.shape\n",
        "        f3_flat = f3.reshape(B, C, H*W).permute(0, 2, 1)\n",
        "        f3_proj = self.linear_v3(f3_flat)\n",
        "        B, C, H, W = f4.shape\n",
        "        f4_flat = f4.reshape(B, C, H*W).permute(0, 2, 1)\n",
        "        f4_proj = self.linear_v4(f4_flat)\n",
        "        visual_proj = torch.cat([f1_proj, f2_proj, f3_proj, f4_proj], dim=1)\n",
        "        word_embeddings, sentence_embedding = text_features\n",
        "        word_proj = self.linear_t_word(word_embeddings)\n",
        "        sent_proj = self.linear_t_sent(sentence_embedding)\n",
        "        text_proj = torch.cat([word_proj, sent_proj], dim=1)\n",
        "        fvt = torch.cat([visual_proj, text_proj], dim=1)\n",
        "        x = f1_proj\n",
        "        for i in range(self.L):\n",
        "            x_trans = x.permute(1, 0, 2)\n",
        "            fvt_trans = fvt.permute(1, 0, 2)\n",
        "            attn_output, _ = self.cross_attn_layers[i](\n",
        "                query=x_trans,\n",
        "                key=fvt_trans,\n",
        "                value=fvt_trans\n",
        "            )\n",
        "            attn_output = attn_output.permute(1, 0, 2)\n",
        "            x = x + attn_output\n",
        "            x = self.cross_attn_norms[i](x)\n",
        "            ffn_output = self.ffn_layers[i](x)\n",
        "            x = x + ffn_output\n",
        "            x = self.ffn_norms[i](x)\n",
        "        for i in range(self.N):\n",
        "            x_trans = x.permute(1, 0, 2)\n",
        "            attn_output, _ = self.self_attn_layers[i](\n",
        "                query=x_trans,\n",
        "                key=x_trans,\n",
        "                value=x_trans\n",
        "            )\n",
        "            attn_output = attn_output.permute(1, 0, 2)\n",
        "            x = x + attn_output\n",
        "            x = self.self_attn_norms[i](x)\n",
        "            ffn_output = self.ffn_layers[i + self.L](x)\n",
        "            x = x + ffn_output\n",
        "            x = self.ffn_norms[i + self.L](x)\n",
        "        return x\n",
        "\n",
        "class MultimodalFusionModule(nn.Module):\n",
        "    def __init__(self, d_model=256):\n",
        "        super(MultimodalFusionModule, self).__init__()\n",
        "        self.learnable_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
        "        nn.init.normal_(self.learnable_token, std=0.02)\n",
        "        self.visual_proj = nn.Linear(256, d_model)\n",
        "        self.text_proj = nn.Linear(768, d_model)\n",
        "\n",
        "        # Set up transformer\n",
        "        transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=8,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1,\n",
        "            activation='relu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=6)\n",
        "\n",
        "    def forward(self, visual_features, text_features):\n",
        "        visual_tokens = self.visual_proj(visual_features)\n",
        "        word_embeddings = text_features[0]\n",
        "        text_tokens = self.text_proj(word_embeddings)\n",
        "        batch_size = visual_tokens.size(0)\n",
        "        learnable_token = self.learnable_token.expand(batch_size, -1, -1)\n",
        "        joint_tokens = torch.cat([learnable_token, visual_tokens, text_tokens], dim=1)\n",
        "        output = self.transformer(joint_tokens)\n",
        "        learnable_output = output[:, 0, :]\n",
        "        return learnable_output\n",
        "\n",
        "class LocalizationModule(nn.Module):\n",
        "    def __init__(self, d_model=256):\n",
        "        super(LocalizationModule, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class RSVGModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RSVGModel, self).__init__()\n",
        "        self.visual_encoder = VisualEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.mlcm = MLCM()\n",
        "        self.multimodal_fusion = MultimodalFusionModule()\n",
        "        self.localization = LocalizationModule()\n",
        "\n",
        "    def forward(self, images, texts):\n",
        "        visual_features = self.visual_encoder(images)\n",
        "        text_features = self.text_encoder(texts)\n",
        "        refined_visual_features = self.mlcm(visual_features, text_features)\n",
        "        fused_features = self.multimodal_fusion(refined_visual_features, text_features)\n",
        "        box_coords = self.localization(fused_features)\n",
        "        return box_coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWrbwd9E9SdA"
      },
      "source": [
        "# 3. LORA Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RWRbBBlbCwv2"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Low-Rank Adaptation layer for efficient fine-tuning of pre-trained models\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, rank=4, alpha=16):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank\n",
        "\n",
        "        # LoRA weights\n",
        "        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))\n",
        "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Low-rank adaptation\n",
        "        return (x @ self.lora_A) @ self.lora_B * self.scaling\n",
        "\n",
        "def apply_lora_to_linear_layers(module, target_names=None, rank=4, alpha=16):\n",
        "    \"\"\"\n",
        "    Apply LoRA to linear layers in a model safely without changing dictionary size during iteration\n",
        "    \"\"\"\n",
        "    if target_names is None:\n",
        "        # Default to common attention projection layers\n",
        "        target_names = ['q_proj', 'k_proj', 'v_proj', 'out_proj', 'query', 'key', 'value']\n",
        "\n",
        "    lora_params = []\n",
        "\n",
        "    # First collect all the modules we want to modify\n",
        "    modules_to_modify = []\n",
        "    for name, submodule in module.named_modules():\n",
        "        if isinstance(submodule, nn.Linear) and any(target in name for target in target_names):\n",
        "            modules_to_modify.append((name, submodule))\n",
        "\n",
        "    # Then apply LoRA without modifying the dictionary during iteration\n",
        "    for name, submodule in modules_to_modify:\n",
        "        in_features, out_features = submodule.in_features, submodule.out_features\n",
        "\n",
        "        # Create a LoRA layer\n",
        "        lora_layer = LoRALayer(in_features, out_features, rank, alpha)\n",
        "\n",
        "        # Store original forward\n",
        "        original_forward = submodule.forward\n",
        "\n",
        "        # Create a new forward method that applies base + LoRA\n",
        "        def create_forward_hook(orig_forward, lora):\n",
        "            def forward_hook(x):\n",
        "                return orig_forward(x) + lora(x)\n",
        "            return forward_hook\n",
        "\n",
        "        # Set the new forward method\n",
        "        submodule.forward = create_forward_hook(original_forward, lora_layer)\n",
        "\n",
        "        # Store the lora_layer as a direct attribute of the parent module\n",
        "        # Use a sanitized name to avoid issues with dots in attribute names\n",
        "        lora_name = f\"{name.replace('.', '_')}_lora\"\n",
        "        setattr(module, lora_name, lora_layer)\n",
        "\n",
        "        # Add parameters to the list of trainable parameters\n",
        "        lora_params.extend(list(lora_layer.parameters()))\n",
        "\n",
        "        print(f\"Applied LoRA to {name}\")\n",
        "\n",
        "    return lora_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jlMc8T89UpX"
      },
      "source": [
        "# 4. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SRVwGFSLKyyd"
      },
      "outputs": [],
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RSVGLoss(nn.Module):\n",
        "    def __init__(self, lambda_giou=1.0):\n",
        "        super(RSVGLoss, self).__init__()\n",
        "        self.smooth_l1 = nn.SmoothL1Loss()\n",
        "        self.lambda_giou = lambda_giou\n",
        "\n",
        "    def forward(self, pred_boxes, target_boxes):\n",
        "        smooth_l1_loss = self.smooth_l1(pred_boxes, target_boxes)\n",
        "        giou_loss = self.generalized_box_iou_loss(pred_boxes, target_boxes)\n",
        "        total_loss = smooth_l1_loss + self.lambda_giou * giou_loss\n",
        "        return total_loss\n",
        "\n",
        "    def generalized_box_iou_loss(self, pred_boxes, target_boxes):\n",
        "        pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
        "        target_area = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n",
        "        left_top = torch.max(pred_boxes[:, :2], target_boxes[:, :2])\n",
        "        right_bottom = torch.min(pred_boxes[:, 2:], target_boxes[:, 2:])\n",
        "        wh = (right_bottom - left_top).clamp(min=0)\n",
        "        intersection = wh[:, 0] * wh[:, 1]\n",
        "        union = pred_area + target_area - intersection\n",
        "        iou = intersection / (union + 1e-7)\n",
        "        enclosing_left_top = torch.min(pred_boxes[:, :2], target_boxes[:, :2])\n",
        "        enclosing_right_bottom = torch.max(pred_boxes[:, 2:], target_boxes[:, 2:])\n",
        "        enclosing_wh = (enclosing_right_bottom - enclosing_left_top).clamp(min=0)\n",
        "        enclosing_area = enclosing_wh[:, 0] * enclosing_wh[:, 1]\n",
        "        giou = iou - (enclosing_area - union) / (enclosing_area + 1e-7)\n",
        "        return 1 - giou.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OUz3lF6HDAGe"
      },
      "outputs": [],
      "source": [
        "class RemoteSensingDataset(Dataset):\n",
        "    def __init__(self, img_dir, ann_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.ann_dir = ann_dir\n",
        "        self.imgs = [f for f in os.listdir(self.img_dir) if f.endswith(('.jpg', '.png'))]\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((640, 640)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.imgs[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        xml_path = os.path.join(self.ann_dir, img_name.replace('.jpg', '.xml').replace('.png', '.xml'))\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            w, h = image.size\n",
        "            image = self.transform(image)\n",
        "\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "            obj = root.find('object')\n",
        "            if obj is None:\n",
        "                return None\n",
        "\n",
        "            grounding_caption_elem = root.find('grounding_caption')\n",
        "            if grounding_caption_elem is not None and grounding_caption_elem.text is not None:\n",
        "                query = grounding_caption_elem.text.strip()\n",
        "                print(f'CAPTION NOT FOUND')\n",
        "            else:\n",
        "                description = obj.find('description')\n",
        "                if description is None or description.text is None:\n",
        "                    query = \"object\"\n",
        "                else:\n",
        "                    query = description.text.strip()\n",
        "\n",
        "            bbox = obj.find('bndbox')\n",
        "            if bbox is None:\n",
        "                return None\n",
        "\n",
        "            x1 = float(bbox.find('xmin').text)\n",
        "            y1 = float(bbox.find('ymin').text)\n",
        "            x2 = float(bbox.find('xmax').text)\n",
        "            y2 = float(bbox.find('ymax').text)\n",
        "\n",
        "            # Normalize coordinates\n",
        "            box = torch.tensor([x1/w, y1/h, x2/w, y2/h], dtype=torch.float32)\n",
        "\n",
        "            return image, query, box\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {img_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "def compute_iou(pred_box, true_box):\n",
        "    \"\"\"\n",
        "    Compute IoU between predicted and ground truth boxes\n",
        "    Boxes are in format [x1, y1, x2, y2]\n",
        "    \"\"\"\n",
        "    # Ensure tensors\n",
        "    if not isinstance(pred_box, torch.Tensor):\n",
        "        pred_box = torch.tensor(pred_box)\n",
        "    if not isinstance(true_box, torch.Tensor):\n",
        "        true_box = torch.tensor(true_box)\n",
        "\n",
        "    # Calculate intersection\n",
        "    x1_inter = torch.max(pred_box[0], true_box[0])\n",
        "    y1_inter = torch.max(pred_box[1], true_box[1])\n",
        "    x2_inter = torch.min(pred_box[2], true_box[2])\n",
        "    y2_inter = torch.min(pred_box[3], true_box[3])\n",
        "\n",
        "    width_inter = torch.max(torch.tensor(0.0), x2_inter - x1_inter)\n",
        "    height_inter = torch.max(torch.tensor(0.0), y2_inter - y1_inter)\n",
        "    area_inter = width_inter * height_inter\n",
        "\n",
        "    # Calculate areas\n",
        "    area_pred = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
        "    area_true = (true_box[2] - true_box[0]) * (true_box[3] - true_box[1])\n",
        "\n",
        "    # Calculate union\n",
        "    area_union = area_pred + area_true - area_inter\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = area_inter / (area_union + 1e-7)\n",
        "\n",
        "    return iou\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Remove None samples\n",
        "    batch = [x for x in batch if x is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "    images, queries, boxes = zip(*batch)\n",
        "    images = torch.stack(images, dim=0)\n",
        "    boxes = torch.stack(boxes, dim=0)\n",
        "    return images, list(queries), boxes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzSxFoMU9XWN"
      },
      "source": [
        "# 5. RSVG_DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EEZQISKVDBMr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.misc import clean_state_dict\n",
        "\n",
        "class RSVG_DINO(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        rsvg_model_path: str,\n",
        "        config_path: str,\n",
        "        groundingdino_weights_path: str\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # RSVG model\n",
        "        print(\"Loading RSVG model...\")\n",
        "        self.rsvg = RSVGModel()\n",
        "        ckpt = torch.load(rsvg_model_path, map_location='cpu')\n",
        "        rsvg_state = ckpt.get(\"model_state_dict\", ckpt)\n",
        "        self.rsvg.load_state_dict(rsvg_state, strict=False)\n",
        "        self.rsvg.eval()\n",
        "        for p in self.rsvg.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        print(\"RSVG model loaded successfully!\")\n",
        "\n",
        "        # GroundingDINO model\n",
        "        print(\"Loading GroundingDINO model...\")\n",
        "        args = SLConfig.fromfile(config_path)\n",
        "        self.groundingdino = build_model(args)\n",
        "        dino_ckpt = torch.load(groundingdino_weights_path, map_location='cpu')\n",
        "        dino_state = clean_state_dict(dino_ckpt.get(\"model\", dino_ckpt))\n",
        "        self.groundingdino.load_state_dict(dino_state, strict=False)\n",
        "        self.groundingdino.eval()\n",
        "        print(\"GroundingDINO model loaded successfully!\")\n",
        "\n",
        "        # Default fallback for tokens\n",
        "        self.default_tokens_positive = [[1]]\n",
        "\n",
        "        # Projection & fusion heads\n",
        "        self.dino_proj = nn.Linear(256, 256)\n",
        "        self.feature_fusion = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.confidence_weighting = nn.Linear(256, 2)\n",
        "        self.enhanced_localization = nn.Linear(256, 4)\n",
        "\n",
        "        # Initialize weights to favor RSVG initially\n",
        "        with torch.no_grad():\n",
        "            if hasattr(self.confidence_weighting, 'bias'):\n",
        "                self.confidence_weighting.bias.data[0] = 1.0  # Higher bias for RSVG\n",
        "                self.confidence_weighting.bias.data[1] = 0.0  # Lower bias for DINO\n",
        "\n",
        "        print(\"RSVG_DINO initialization complete!\")\n",
        "\n",
        "        # Debug mode\n",
        "        self.debug = True\n",
        "\n",
        "    def forward(self, images, texts):\n",
        "        B = images.shape[0]\n",
        "        device = images.device\n",
        "\n",
        "        if self.debug:\n",
        "            print(\"\\n==== DEBUG: RSVG_DINO.forward ====\")\n",
        "            print(f\"Batch size: {B}\")\n",
        "            print(f\"Images shape: {images.shape}\")\n",
        "            print(f\"Texts type: {type(texts)}, length: {len(texts)}\")\n",
        "            for i, t in enumerate(texts[:min(3, len(texts))]):  # Print first few\n",
        "               print(f\"  Text {i}: {t}\")\n",
        "            if len(texts) > 3:\n",
        "                print(f\"  ... and {len(texts) - 3} more\")\n",
        "\n",
        "           # Send models to device\n",
        "        self.rsvg.to(device)\n",
        "        self.groundingdino.to(device)\n",
        "\n",
        "        # RSVG\n",
        "        if self.debug:\n",
        "            print(\"\\n--- Processing RSVG pipeline ---\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get inputs for RSVG\n",
        "            if isinstance(texts[0], dict) and \"caption\" in texts[0]:\n",
        "                rsvg_texts = [t[\"caption\"] for t in texts]\n",
        "            else:\n",
        "                rsvg_texts = texts\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"RSVG texts: {rsvg_texts[:min(3, len(rsvg_texts))]}\")\n",
        "\n",
        "            # Process through RSVG pipeline\n",
        "            vf = self.rsvg.visual_encoder(images)\n",
        "            tf = self.rsvg.text_encoder(rsvg_texts)\n",
        "            rv = self.rsvg.mlcm(vf, tf)\n",
        "            rsvg_feats = self.rsvg.multimodal_fusion(rv, tf)\n",
        "            rsvg_boxes = self.rsvg.localization(rsvg_feats)\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"RSVG features shape: {rsvg_feats.shape}\")\n",
        "                print(f\"RSVG boxes shape: {rsvg_boxes.shape}\")\n",
        "                print(f\"RSVG sample boxes: {rsvg_boxes[0]}\")\n",
        "\n",
        "        # GROUNDING DINO\n",
        "        if self.debug:\n",
        "            print(\"\\n--- Processing GroundingDINO pipeline ---\")\n",
        "\n",
        "        try:\n",
        "            # Format queries with guaranteed valid token spans\n",
        "            # from transformers import BertTokenizerFast\n",
        "            tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "            dino_texts = []\n",
        "            for cap in texts:\n",
        "                # If input is dict (from custom collate), extract \"caption\"\n",
        "                if isinstance(cap, dict) and \"caption\" in cap:\n",
        "                    caption = cap[\"caption\"]\n",
        "                else:\n",
        "                    caption = cap\n",
        "                ids = tokenizer(caption, add_special_tokens=True).input_ids\n",
        "                seq_len = len(ids)\n",
        "                tokens_pos = list(range(1, seq_len - 1))\n",
        "                if not tokens_pos:\n",
        "                    tokens_pos = [1]\n",
        "                dino_texts.append({\n",
        "                    \"caption\": caption,\n",
        "                    \"tokens_positive\": [tokens_pos],\n",
        "                })\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"Formatted {len(dino_texts)} texts for GroundingDINO\")\n",
        "                print(f\"Sample formatted text: {dino_texts[0]}\")\n",
        "\n",
        "            # IMPORTANT: Add special attribute to help bertwarper\n",
        "            self.groundingdino.specical_tokens = [q[\"tokens_positive\"][0] for q in dino_texts]\n",
        "\n",
        "            if self.debug:\n",
        "                print(\"Set self.groundingdino.specical_tokens to:\")\n",
        "                for i, tok in enumerate(self.groundingdino.specical_tokens[:min(3, len(self.groundingdino.specical_tokens))]):\n",
        "                    print(f\"  Sample {i}: {tok}\")\n",
        "\n",
        "            # CRITICAL DEBUGGING POINT - Before actual GroundingDINO forward\n",
        "            if self.debug:\n",
        "                print(\"\\n>>> About to call groundingdino forward <<<\")\n",
        "                print(f\"groundingdino input image shape: {images.shape}\")\n",
        "                print(f\"groundingdino input texts: {len(dino_texts)} items\")\n",
        "                try:\n",
        "                    model_state = {\n",
        "                        \"has_tokenizer\": hasattr(self.groundingdino, \"tokenizer\"),\n",
        "                        \"has_special_tokens\": hasattr(self.groundingdino, \"specical_tokens\"),\n",
        "                        \"tokenizer_type\": type(self.groundingdino.tokenizer).__name__ if hasattr(self.groundingdino, \"tokenizer\") else None,\n",
        "                    }\n",
        "                    print(f\"Model state checks: {model_state}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during model inspection: {e}\")\n",
        "\n",
        "            # Process through GroundingDINO\n",
        "            dino_out = self.groundingdino(images, dino_texts)\n",
        "\n",
        "            if self.debug:\n",
        "                print(\"\\n>>> GroundingDINO forward succeeded! <<<\")\n",
        "                print(f\"Output keys: {list(dino_out.keys())}\")\n",
        "\n",
        "            # Extract outputs\n",
        "            logits = dino_out[\"pred_logits\"]\n",
        "            boxes = dino_out[\"pred_boxes\"]\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"Pred logits shape: {logits.shape}\")\n",
        "                print(f\"Pred boxes shape: {boxes.shape}\")\n",
        "\n",
        "            # Get feature embeddings\n",
        "            if \"hidden_states\" in dino_out:\n",
        "                df = dino_out[\"hidden_states\"][-1][:, 0, :]\n",
        "                if self.debug:\n",
        "                    print(f\"Using hidden_states for features, shape: {df.shape}\")\n",
        "            elif \"encoder_hidden_states\" in dino_out:\n",
        "                df = dino_out[\"encoder_hidden_states\"][-1][:, 0, :]\n",
        "                if self.debug:\n",
        "                    print(f\"Using encoder_hidden_states for features, shape: {df.shape}\")\n",
        "            else:\n",
        "                df = torch.zeros(B, 256, device=device)\n",
        "                if self.debug:\n",
        "                    print(\"No hidden states found, using zeros\")\n",
        "\n",
        "            # Process boxes and scores\n",
        "            box_list, score_list = [], []\n",
        "            for b in range(B):\n",
        "                scores = F.softmax(logits[b, :, 0], dim=0)\n",
        "                if self.debug and b == 0:\n",
        "                    print(f\"Sample scores shape: {scores.shape}\")\n",
        "                    print(f\"Sample scores values: {scores[:5]}\")  # First 5 scores\n",
        "\n",
        "                if scores.numel() > 0:\n",
        "                    # Get best prediction\n",
        "                    i = scores.argmax()\n",
        "                    s = scores[i]\n",
        "                    c = boxes[b, i]\n",
        "\n",
        "                    # Convert from center-size to corners\n",
        "                    x1 = c[0] - c[2] / 2\n",
        "                    y1 = c[1] - c[3] / 2\n",
        "                    x2 = c[0] + c[2] / 2\n",
        "                    y2 = c[1] + c[3] / 2\n",
        "\n",
        "                    box_list.append(torch.stack([x1, y1, x2, y2], dim=0))\n",
        "                    score_list.append(s)\n",
        "\n",
        "                    if self.debug and b == 0:\n",
        "                        print(f\"Best box for sample 0: {[x1.item(), y1.item(), x2.item(), y2.item()]}\")\n",
        "                        print(f\"Confidence score: {s.item()}\")\n",
        "                else:\n",
        "                    # Default box (full image) with zero confidence\n",
        "                    box_list.append(torch.tensor([0, 0, 1, 1], device=device))\n",
        "                    score_list.append(torch.tensor(0.0, device=device))\n",
        "\n",
        "                    if self.debug and b == 0:\n",
        "                        print(\"Using default box [0,0,1,1] with zero confidence\")\n",
        "\n",
        "            # Stack results\n",
        "            dino_boxes = torch.stack(box_list)\n",
        "            dino_scores = torch.stack(score_list)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug:\n",
        "                print(f\"\\nERROR in GroundingDINO processing: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "            # Use default values on error\n",
        "            df = torch.zeros(B, 256, device=device)\n",
        "            dino_boxes = torch.zeros(B, 4, device=device)\n",
        "            dino_scores = torch.zeros(B, device=device)\n",
        "\n",
        "        # FEATURE FUSION & OUTPUT\n",
        "        if self.debug:\n",
        "            print(\"\\n--- Processing Feature Fusion ---\")\n",
        "\n",
        "        df_proj = self.dino_proj(df)\n",
        "        fused_in = torch.cat([rsvg_feats, df_proj], dim=1)\n",
        "        fused_feats = self.feature_fusion(fused_in)\n",
        "        weights = F.softmax(self.confidence_weighting(fused_feats), dim=1)\n",
        "        enhanced_bxes = self.enhanced_localization(fused_feats)\n",
        "\n",
        "        # Weighted average of boxes based on confidence\n",
        "        weighted_bxes = weights[:, 0:1] * rsvg_boxes + weights[:, 1:2] * dino_boxes\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Fusion weights: RSVG={weights[0, 0].item():.4f}, DINO={weights[0, 1].item():.4f}\")\n",
        "            print(f\"Final boxes sample: {weighted_bxes[0]}\")\n",
        "            print(\"==== END DEBUG: RSVG_DINO.forward ====\\n\")\n",
        "\n",
        "        return {\n",
        "            \"boxes\": weighted_bxes,\n",
        "            \"enhanced_boxes\": enhanced_bxes,\n",
        "            \"rsvg_boxes\": rsvg_boxes,\n",
        "            \"dino_boxes\": dino_boxes,\n",
        "            \"dino_scores\": dino_scores,\n",
        "            \"confidence_weights\": weights,\n",
        "            \"fused_features\": fused_feats\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3244V_aA9ao8"
      },
      "source": [
        "# 6. Training Loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bt-6NpA-DXQt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_rsvgdino_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    device,\n",
        "    checkpoint_dir,\n",
        "    num_epochs\n",
        "):\n",
        "    \"\"\"\n",
        "    model         : your RSVG_DINO\n",
        "    train_loader  : DataLoader yielding (images, texts, targets)\n",
        "    criterion     : RSVGLoss(pred_boxes, target_boxes)\n",
        "    optimizer     : as you already built it\n",
        "    device        : torch.device\n",
        "    checkpoint_dir: path to save checkpoints\n",
        "    num_epochs    : int\n",
        "    \"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    history = {\"loss\": []}\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"[Epoch {epoch}/{num_epochs}]\", leave=False)\n",
        "        for batch_idx, (images, texts, targets) in enumerate(loop, start=1):\n",
        "            # ---- 1) move images to device ----\n",
        "            images = images.to(device)\n",
        "\n",
        "            # ---- 2) pull out target_boxes ----\n",
        "            if isinstance(targets, dict) and \"boxes\" in targets:\n",
        "                # your collate returned a dict of batched tensors\n",
        "                target_boxes = targets[\"boxes\"].to(device)\n",
        "\n",
        "            elif torch.is_tensor(targets):\n",
        "                # collate returned a single tensor of shape [B,4]\n",
        "                target_boxes = targets.to(device)\n",
        "\n",
        "            elif isinstance(targets, (list, tuple)) and torch.is_tensor(targets[0]):\n",
        "                # collate returned list of per-sample box tensors [4] -> stack into [B,4]\n",
        "                target_boxes = torch.stack([t.to(device) for t in targets], dim=0)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unrecognized targets format: {type(targets)}\")\n",
        "\n",
        "            # ---- 3) forward + loss ----\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, texts)\n",
        "            pred_boxes = outputs[\"boxes\"]\n",
        "\n",
        "            loss = criterion(pred_boxes, target_boxes)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # ---- 4) logging ----\n",
        "            running_loss += loss.item()\n",
        "            if batch_idx % 10 == 0:\n",
        "                avg_batch_loss = running_loss / batch_idx\n",
        "                print(f\"[Epoch {epoch}] Batch {batch_idx}/{len(train_loader)} — avg loss: {avg_batch_loss:.4f}\")\n",
        "            loop.set_postfix(loss=running_loss / batch_idx)\n",
        "\n",
        "        # end of epoch\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        history[\"loss\"].append(avg_loss)\n",
        "        print(f\"Epoch {epoch}/{num_epochs} — avg loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # save every 5 epochs\n",
        "        if epoch % 5 == 0:\n",
        "            ckpt_name = f\"rsvgdino_{epoch}.pth\"\n",
        "            ckpt_path = os.path.join(checkpoint_dir, ckpt_name)\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            print(f\"Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbHnX-dwDfNW",
        "outputId": "2665c6c3-5a32-4ea1-b0d3-776b4ef176e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Training dataset contains 12340 samples\n",
            "Loading RSVG model...\n",
            "RSVG model loaded successfully!\n",
            "Loading GroundingDINO model...\n",
            "final text_encoder_type: bert-base-uncased\n",
            "GroundingDINO model loaded successfully!\n",
            "RSVG_DINO initialization complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "# 1. Configuration\n",
        "batch_size = 14\n",
        "num_epochs = 20\n",
        "\n",
        "# 2. paths to pretrained RSVG & DINO\n",
        "rsvg_model_path            = f\"/{HOME}/drive/MyDrive/Checkpoints/rsvg_checkpoint_epoch_150.pth\"\n",
        "config_path                = f\"/{HOME}/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "groundingdino_weights_path = f\"/{HOME}/weights/groundingdino_swint_ogc.pth\"\n",
        "\n",
        "# 3. where to save your new checkpoints\n",
        "checkpoint_dir = f\"{HOME}/drive/MyDrive/Checkpoints2\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# 3. Data directories\n",
        "train_img_dir = f\"{HOME}/drive/MyDrive/DATASET/train_data/train_images\"\n",
        "train_ann_dir = f\"{HOME}/drive/MyDrive/DATASET/train_data/train_annotations\"\n",
        "\n",
        "# 5. Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 6. Dataset & DataLoader\n",
        "train_dataset = RemoteSensingDataset(\n",
        "    img_dir=train_img_dir,\n",
        "    ann_dir=train_ann_dir\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=custom_collate_fn\n",
        ")\n",
        "print(f\"Training dataset contains {len(train_dataset)} samples\")\n",
        "\n",
        "# 7. Model\n",
        "model = RSVG_DINO(\n",
        "    rsvg_model_path=rsvg_model_path,\n",
        "    config_path=config_path,\n",
        "    groundingdino_weights_path=groundingdino_weights_path\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# 8 Optimizer\n",
        "backbone_params = []\n",
        "text_params     = []\n",
        "fusion_params   = []\n",
        "lora_params     = []\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if not param.requires_grad:\n",
        "        continue\n",
        "\n",
        "    # LoRA adapters\n",
        "    if \"lora\" in name:\n",
        "        lora_params.append(param)\n",
        "\n",
        "    # Vision & DINO backbones\n",
        "    elif name.startswith(\"rsvg.visual_encoder.backbone\") \\\n",
        "      or name.startswith(\"groundingdino.backbone\") \\\n",
        "      or name.startswith(\"groundingdino.input_proj\"):\n",
        "        backbone_params.append(param)\n",
        "\n",
        "    # Text encoder (BERT)\n",
        "    elif name.startswith(\"rsvg.text_encoder.bert\") \\\n",
        "      or name.startswith(\"groundingdino.bert\"):\n",
        "        text_params.append(param)\n",
        "\n",
        "    # Everything else new: fusion, heads, MLCM, projections\n",
        "    else:\n",
        "        fusion_params.append(param)\n",
        "\n",
        "optimizer = AdamW([\n",
        "    { \"params\": backbone_params, \"lr\": 5e-6 },   # backbone\n",
        "    { \"params\": text_params,     \"lr\": 1e-5 },   # text\n",
        "    { \"params\": fusion_params,   \"lr\": 2e-5 },   # fusion heads\n",
        "    { \"params\": lora_params,     \"lr\": 8e-5 },   # LoRA\n",
        "], weight_decay=1e-2)\n",
        "\n",
        "\n",
        "# 9. Loss function\n",
        "criterion = RSVGLoss(lambda_giou=1.0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== DEBUG: RSVG_DINO.forward ====\n",
            "Batch size: 14\n",
            "Images shape: torch.Size([14, 3, 640, 640])\n",
            "Texts type: <class 'list'>, length: 14\n",
            "  Text 0: The baseball field in the middle\n",
            "  Text 1: A tennis court at the bottom\n",
            "  Text 2: A stadium has the ground track field in the middle\n",
            "  ... and 11 more\n",
            "\n",
            "--- Processing RSVG pipeline ---\n",
            "RSVG texts: ['The baseball field in the middle', 'A tennis court at the bottom', 'A stadium has the ground track field in the middle']\n",
            "RSVG features shape: torch.Size([14, 256])\n",
            "RSVG boxes shape: torch.Size([14, 4])\n",
            "RSVG sample boxes: tensor([0.2602, 0.2436, 0.5965, 0.5994])\n",
            "\n",
            "--- Processing GroundingDINO pipeline ---\n",
            "Formatted 14 texts for GroundingDINO\n",
            "Sample formatted text: {'caption': 'The baseball field in the middle', 'tokens_positive': [[1, 2, 3, 4, 5, 6]]}\n",
            "Set self.groundingdino.specical_tokens to:\n",
            "  Sample 0: [1, 2, 3, 4, 5, 6]\n",
            "  Sample 1: [1, 2, 3, 4, 5, 6]\n",
            "  Sample 2: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "\n",
            ">>> About to call groundingdino forward <<<\n",
            "groundingdino input image shape: torch.Size([14, 3, 640, 640])\n",
            "groundingdino input texts: 14 items\n",
            "Model state checks: {'has_tokenizer': True, 'has_special_tokens': True, 'tokenizer_type': 'BertTokenizerFast'}\n",
            "[DEBUG] About to stack cate_to_token_mask_list[0] with length 0\n",
            "[DEBUG] input_ids[0]: [101, 1996, 3598, 2492, 1999, 1996, 2690, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[1] with length 0\n",
            "[DEBUG] input_ids[1]: [101, 1037, 5093, 2457, 2012, 1996, 3953, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[2] with length 0\n",
            "[DEBUG] input_ids[2]: [101, 1037, 3346, 2038, 1996, 2598, 2650, 2492, 1999, 1996, 2690, 102, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[3] with length 0\n",
            "[DEBUG] input_ids[3]: [101, 1037, 3598, 2492, 2006, 1996, 3356, 2187, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[4] with length 0\n",
            "[DEBUG] input_ids[4]: [101, 1996, 13297, 2003, 2714, 1999, 2946, 2000, 1996, 8352, 2235, 5527, 4951, 2006, 1996, 2157, 102]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[5] with length 0\n",
            "[DEBUG] input_ids[5]: [101, 1037, 4316, 2012, 1996, 3953, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[6] with length 0\n",
            "[DEBUG] input_ids[6]: [101, 1996, 13297, 2003, 2682, 1996, 13297, 2006, 1996, 2187, 102, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[7] with length 0\n",
            "[DEBUG] input_ids[7]: [101, 1996, 3897, 3199, 1999, 1996, 2690, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[8] with length 0\n",
            "[DEBUG] input_ids[8]: [101, 1996, 3455, 2457, 2006, 1996, 2327, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[9] with length 0\n",
            "[DEBUG] input_ids[9]: [101, 1996, 13297, 2006, 1996, 2327, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[10] with length 0\n",
            "[DEBUG] input_ids[10]: [101, 1037, 2312, 5477, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[11] with length 0\n",
            "[DEBUG] input_ids[11]: [101, 1996, 11720, 9565, 2276, 1999, 1996, 2690, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[12] with length 0\n",
            "[DEBUG] input_ids[12]: [101, 1037, 25367, 2006, 1996, 2327, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[13] with length 0\n",
            "[DEBUG] input_ids[13]: [101, 1037, 3897, 5477, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            ">>> GroundingDINO forward succeeded! <<<\n",
            "Output keys: ['pred_logits', 'pred_boxes']\n",
            "Pred logits shape: torch.Size([14, 900, 256])\n",
            "Pred boxes shape: torch.Size([14, 900, 4])\n",
            "No hidden states found, using zeros\n",
            "Sample scores shape: torch.Size([900])\n",
            "Sample scores values: tensor([0.0036, 0.0052, 0.0045, 0.0036, 0.0032], grad_fn=<SliceBackward0>)\n",
            "Best box for sample 0: [0.0001754164695739746, -0.00013336539268493652, 1.0000171661376953, 0.9998480081558228]\n",
            "Confidence score: 0.00518634682521224\n",
            "\n",
            "--- Processing Feature Fusion ---\n",
            "Fusion weights: RSVG=0.6414, DINO=0.3586\n",
            "Final boxes sample: tensor([0.1670, 0.1562, 0.7412, 0.7430], grad_fn=<SelectBackward0>)\n",
            "==== END DEBUG: RSVG_DINO.forward ====\n",
            "\n",
            " BOXES: tensor([[0.1670, 0.1562, 0.7412, 0.7430],\n",
            "        [0.2595, 0.4067, 0.8058, 0.9714],\n",
            "        [0.3298, 0.1867, 0.7312, 0.6821],\n",
            "        [0.0637, 0.0671, 0.5624, 0.5862],\n",
            "        [0.5213, 0.1349, 0.8267, 0.4172],\n",
            "        [0.1888, 0.4023, 0.7954, 1.0057],\n",
            "        [0.0594, 0.0055, 0.6486, 0.5942],\n",
            "        [0.1646, 0.1446, 0.9026, 0.8923],\n",
            "        [0.3614, 0.0973, 0.6609, 0.4217],\n",
            "        [0.3286, 0.1077, 0.6523, 0.3938],\n",
            "        [0.1743, 0.2069, 0.8382, 0.8438],\n",
            "        [0.1884, 0.2201, 0.7989, 0.8146],\n",
            "        [0.3623, 0.0803, 0.6627, 0.3281],\n",
            "        [0.1972, 0.1936, 0.8554, 0.8900]], grad_fn=<AddBackward0>)\n",
            " ENHANCED BOXES: tensor([[ 0.0268, -0.1907,  0.0468,  0.1498],\n",
            "        [ 0.0132, -0.1653,  0.0818,  0.1279],\n",
            "        [ 0.0548, -0.1637,  0.0334,  0.1770],\n",
            "        [ 0.0201, -0.0978,  0.1599,  0.0870],\n",
            "        [ 0.0715, -0.2180, -0.0216,  0.1721],\n",
            "        [ 0.0212, -0.1406,  0.0903,  0.1379],\n",
            "        [ 0.0039, -0.1423,  0.1016,  0.1580],\n",
            "        [ 0.0509, -0.1372,  0.0395,  0.1505],\n",
            "        [ 0.0054, -0.1849, -0.0133,  0.1435],\n",
            "        [ 0.0028, -0.1663, -0.0215,  0.1398],\n",
            "        [ 0.0423, -0.1603,  0.0427,  0.1413],\n",
            "        [ 0.0357, -0.1774,  0.0447,  0.1406],\n",
            "        [-0.0011, -0.1413, -0.0282,  0.1335],\n",
            "        [ 0.0382, -0.1652,  0.0459,  0.1497]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Assume you have already loaded your model as `model`\n",
        "# and have a batch of images and queries (texts)\n",
        "\n",
        "# Example: get a batch from your DataLoader\n",
        "batch = next(iter(train_loader))  # or test_loader\n",
        "images, queries, boxes = batch  # boxes are optional for inference\n",
        "\n",
        "# Move images to device\n",
        "images = images.to(device)\n",
        "\n",
        "# Call the model directly\n",
        "outputs = model(images, queries)\n",
        "\n",
        "# outputs is a dict with keys: \"boxes\", \"enhanced_boxes\", etc.\n",
        "print(f' BOXES: {outputs[\"boxes\"]}')\n",
        "print(f' ENHANCED BOXES: {outputs[\"enhanced_boxes\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TTkW4G79a3uq"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aff14a7ecb994659941f61f147c90b85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "[Epoch 1/20]:   0%|          | 0/882 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== DEBUG: RSVG_DINO.forward ====\n",
            "Batch size: 14\n",
            "Images shape: torch.Size([14, 3, 640, 640])\n",
            "Texts type: <class 'list'>, length: 14\n",
            "  Text 0: A small gray slender overpass\n",
            "  Text 1: The windmill on the upper left\n",
            "  Text 2: The baseball field on the left\n",
            "  ... and 11 more\n",
            "\n",
            "--- Processing RSVG pipeline ---\n",
            "RSVG texts: ['A small gray slender overpass', 'The windmill on the upper left', 'The baseball field on the left']\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Kick off training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trained_model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rsvgdino_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[19], line 53\u001b[0m, in \u001b[0;36mtrain_rsvgdino_model\u001b[0;34m(model, train_loader, criterion, optimizer, device, checkpoint_dir, num_epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# ---- 3) forward + loss ----\u001b[39;00m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 53\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_boxes, target_boxes)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[18], line 96\u001b[0m, in \u001b[0;36mRSVG_DINO.forward\u001b[0;34m(self, images, texts)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRSVG texts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrsvg_texts[:\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(rsvg_texts))]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Process through RSVG pipeline\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsvg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m tf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrsvg\u001b[38;5;241m.\u001b[39mtext_encoder(rsvg_texts)\n\u001b[1;32m     98\u001b[0m rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrsvg\u001b[38;5;241m.\u001b[39mmlcm(vf, tf)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[13], line 14\u001b[0m, in \u001b[0;36mVisualEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv6_1(f1))\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/timm/models/_features.py:345\u001b[0m, in \u001b[0;36mFeatureListNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m (List[torch\u001b[38;5;241m.\u001b[39mTensor]):\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/timm/models/_features.py:299\u001b[0m, in \u001b[0;36mFeatureDictNet._collect\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    297\u001b[0m     x \u001b[38;5;241m=\u001b[39m module(x) \u001b[38;5;28;01mif\u001b[39;00m first_or_last_module \u001b[38;5;28;01melse\u001b[39;00m checkpoint(module, x)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n\u001b[1;32m    302\u001b[0m     out_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers[name]\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/timm/models/resnet.py:219\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    216\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact2(x)\n\u001b[1;32m    217\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maa(x)\n\u001b[0;32m--> 219\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(x)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Kick off training\n",
        "trained_model, history = train_rsvgdino_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    num_epochs=num_epochs\n",
        ")\n",
        "\n",
        "print(\"Training completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v08mnlNXqXS5",
        "outputId": "8e8b08bb-5626-4148-f03d-c1febbf6c0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Loading RSVG model...\n",
            "RSVG model loaded successfully!\n",
            "Loading GroundingDINO model...\n",
            "final text_encoder_type: bert-base-uncased\n",
            "GroundingDINO model loaded successfully!\n",
            "RSVG_DINO initialization complete!\n",
            "Loaded model from //Users/danyaalsadiq/Downloads/DL_Group_5/drive/MyDrive/Checkpoints2/rsvgdino_15.pth\n",
            "Starting testing...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da83128022414a868eda2bb1a0f45bd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing:   0%|          | 0/211 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== DEBUG: RSVG_DINO.forward ====\n",
            "Batch size: 16\n",
            "Images shape: torch.Size([16, 3, 640, 640])\n",
            "Texts type: <class 'list'>, length: 16\n",
            "  Text 0: A storage tank on the left\n",
            "  Text 1: A small blue and gray frustum of a cone chimney\n",
            "  Text 2: A windmill on the upper left\n",
            "  ... and 13 more\n",
            "\n",
            "--- Processing RSVG pipeline ---\n",
            "RSVG texts: ['A storage tank on the left', 'A small blue and gray frustum of a cone chimney', 'A windmill on the upper left']\n",
            "RSVG features shape: torch.Size([16, 256])\n",
            "RSVG boxes shape: torch.Size([16, 4])\n",
            "RSVG sample boxes: tensor([0.0621, 0.3297, 0.3020, 0.6001])\n",
            "\n",
            "--- Processing GroundingDINO pipeline ---\n",
            "Formatted 16 texts for GroundingDINO\n",
            "Sample formatted text: {'caption': 'A storage tank on the left', 'tokens_positive': [[1, 2, 3, 4, 5, 6]]}\n",
            "Set self.groundingdino.specical_tokens to:\n",
            "  Sample 0: [1, 2, 3, 4, 5, 6]\n",
            "  Sample 1: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "  Sample 2: [1, 2, 3, 4, 5, 6]\n",
            "\n",
            ">>> About to call groundingdino forward <<<\n",
            "groundingdino input image shape: torch.Size([16, 3, 640, 640])\n",
            "groundingdino input texts: 16 items\n",
            "Model state checks: {'has_tokenizer': True, 'has_special_tokens': True, 'tokenizer_type': 'BertTokenizerFast'}\n",
            "[DEBUG] About to stack cate_to_token_mask_list[0] with length 0\n",
            "[DEBUG] input_ids[0]: [101, 1037, 5527, 4951, 2006, 1996, 2187, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[1] with length 0\n",
            "[DEBUG] input_ids[1]: [101, 1037, 2235, 2630, 1998, 3897, 10424, 19966, 2819, 1997, 1037, 13171, 17321, 102, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[2] with length 0\n",
            "[DEBUG] input_ids[2]: [101, 1037, 25367, 2006, 1996, 3356, 2187, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[3] with length 0\n",
            "[DEBUG] input_ids[3]: [101, 1996, 3455, 2457, 2006, 1996, 2187, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[4] with length 0\n",
            "[DEBUG] input_ids[4]: [101, 1996, 2312, 5439, 2492, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[5] with length 0\n",
            "[DEBUG] input_ids[5]: [101, 1996, 2317, 1998, 3897, 11720, 9565, 2276, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[6] with length 0\n",
            "[DEBUG] input_ids[6]: [101, 1037, 2312, 5527, 4951, 1999, 1996, 2690, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[7] with length 0\n",
            "[DEBUG] input_ids[7]: [101, 1996, 3598, 2492, 2003, 2006, 1996, 3356, 2187, 1997, 1996, 4589, 1998, 2417, 2598, 2650, 2492, 102, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[8] with length 0\n",
            "[DEBUG] input_ids[8]: [101, 1037, 17321, 2006, 1996, 2896, 2187, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[9] with length 0\n",
            "[DEBUG] input_ids[9]: [101, 1996, 3346, 2003, 2006, 1996, 3356, 2187, 1997, 1996, 2958, 2012, 1996, 3953, 102, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[10] with length 0\n",
            "[DEBUG] input_ids[10]: [101, 1996, 3346, 2003, 2006, 1996, 3356, 2157, 1997, 1996, 9242, 4714, 2598, 2650, 2492, 2006, 1996, 2187, 102]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[11] with length 0\n",
            "[DEBUG] input_ids[11]: [101, 1037, 2146, 3199, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[12] with length 0\n",
            "[DEBUG] input_ids[12]: [101, 1037, 2911, 2006, 1996, 2187, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[13] with length 0\n",
            "[DEBUG] input_ids[13]: [101, 1996, 2665, 2598, 2650, 2492, 1999, 1996, 2690, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[14] with length 0\n",
            "[DEBUG] input_ids[14]: [101, 1037, 3199, 1999, 1996, 2690, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[DEBUG] About to stack cate_to_token_mask_list[15] with length 0\n",
            "[DEBUG] input_ids[15]: [101, 1996, 4714, 3346, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def test_model(model, test_loader, device):\n",
        "    print(\"Starting testing...\")\n",
        "    model.eval()\n",
        "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "    correct_by_threshold = {t: 0 for t in thresholds}\n",
        "    total_iou = 0\n",
        "    total_intersection = 0\n",
        "    total_union = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            if batch is None:\n",
        "                continue\n",
        "            images, queries, boxes = batch\n",
        "            images, boxes = images.to(device), boxes.to(device)\n",
        "            preds = model(images, queries)\n",
        "            for pred_box, true_box in zip(preds[\"boxes\"].cpu().numpy(), boxes.cpu().numpy()):\n",
        "                iou = compute_iou(pred_box, true_box)\n",
        "                total_iou += iou\n",
        "                x1_pred, y1_pred, x2_pred, y2_pred = pred_box\n",
        "                x1_true, y1_true, x2_true, y2_true = true_box\n",
        "                intersection_width = max(0, min(x2_pred, x2_true) - max(x1_pred, x1_true))\n",
        "                intersection_height = max(0, min(y2_pred, y2_true) - max(y1_pred, y1_true))\n",
        "                intersection = intersection_width * intersection_height\n",
        "                pred_area = (x2_pred - x1_pred) * (y2_pred - y1_pred)\n",
        "                true_area = (x2_true - x1_true) * (y2_true - y1_true)\n",
        "                union = pred_area + true_area - intersection\n",
        "                total_intersection += intersection\n",
        "                total_union += union\n",
        "                for t in thresholds:\n",
        "                    if iou >= t:\n",
        "                        correct_by_threshold[t] += 1\n",
        "    dataset_size = len(test_loader.dataset)\n",
        "    mean_iou = total_iou / dataset_size\n",
        "    cum_iou = total_intersection / total_union\n",
        "    precision_at_threshold = {t: correct_by_threshold[t] / dataset_size for t in thresholds}\n",
        "    print(f\"Test Results:\")\n",
        "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
        "    print(f\"Cumulative IoU: {cum_iou:.4f}\")\n",
        "    for t in thresholds:\n",
        "        precision = precision_at_threshold[t]\n",
        "        print(f\"Pr@{t}: {precision:.4f}\")\n",
        "    results = {\n",
        "        'mean_iou': mean_iou,\n",
        "        'cum_iou': cum_iou,\n",
        "        'precision_at_threshold': precision_at_threshold\n",
        "    }\n",
        "    return results\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "test_img_dir = f\"/{HOME}/drive/MyDrive/DATASET/test_data/test_images\"\n",
        "test_ann_dir = f\"/{HOME}/drive/MyDrive/DATASET/test_data/test_annotations\"\n",
        "checkpoint_dir = f\"/{HOME}/drive/MyDrive/Checkpoints2\"\n",
        "model_path = os.path.join(checkpoint_dir, 'rsvgdino_15.pth')\n",
        "test_dataset = RemoteSensingDataset(test_img_dir, test_ann_dir)\n",
        "\n",
        "rsvg_model_path            = f\"/{HOME}/drive/MyDrive/Checkpoints/rsvg_checkpoint_epoch_150.pth\"\n",
        "config_path                = f\"/{HOME}/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "groundingdino_weights_path = f\"/{HOME}/weights/groundingdino_swint_ogc.pth\"\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
        "model = RSVG_DINO(rsvg_model_path= rsvg_model_path,\n",
        "                  config_path= config_path,\n",
        "                  groundingdino_weights_path= groundingdino_weights_path\n",
        "                  ).to(device)\n",
        "\n",
        "\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "print(f\"Loaded model from {model_path}\")\n",
        "test_results = test_model(model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
