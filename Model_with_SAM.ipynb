{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5RayCvi9drR"
      },
      "source": [
        "# Deliverable 4\n",
        "----\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqvVtBXB9NP5"
      },
      "source": [
        "# 1. Import and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd6H8a2p6byY",
        "outputId": "19340ea6-f2a7-447e-c065-64c5e19debc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv63B9o986kt",
        "outputId": "9de59f4f-f1fe-4fab-944d-6f239fb1d8a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HOME is /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(f'HOME is {HOME}')\n",
        "import sys\n",
        "import torch\n",
        "!rm -rf /content/GroundingDINO\n",
        "!rm -rf /content/requirements.txt\n",
        "!rm -rf /content/groundingdino_swint_ogc.pth\n",
        "!rm -rf /content/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj1P0qFw5o3w",
        "outputId": "c453f926-517e-47f7-d2f9-52214de39281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'GroundingDINO'...\n",
            "remote: Enumerating objects: 463, done.\u001b[K\n",
            "remote: Counting objects: 100% (240/240), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 463 (delta 175), reused 135 (delta 135), pack-reused 223 (from 1)\u001b[K\n",
            "Receiving objects: 100% (463/463), 12.87 MiB | 35.72 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n",
            "/content/GroundingDINO\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.51.3)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.43.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.0.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
            "Requirement already satisfied: supervision>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.25.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 2)) (11.2.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->-r requirements.txt (line 5)) (4.3.7)\n",
            "Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->-r requirements.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->-r requirements.txt (line 9)) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->-r requirements.txt (line 9)) (3.10.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->-r requirements.txt (line 9)) (1.15.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->-r requirements.txt (line 9)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->-r requirements.txt (line 9)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->-r requirements.txt (line 9)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->-r requirements.txt (line 9)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->-r requirements.txt (line 9)) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision>=0.22.0->-r requirements.txt (line 9)) (1.17.0)\n",
            "HOME is /content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd './GroundingDINO'\n",
        "!pip install -r requirements.txt\n",
        "print(f'HOME is {HOME}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x8_Ca9M60tj",
        "outputId": "a7b3188b-8a4c-41e6-8e6c-cb11c148d4c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sed: can't read s/value\\.type()/value.scalar_type()/g: No such file or directory\n",
            "sed: can't read s/::detail::scalar_type(the_type)/the_type/g: No such file or directory\n",
            "sed: can't read s/value\\.scalar_type()\\.is_cuda()/value.is_cuda()/g: No such file or directory\n",
            "Fully patched CUDA file for modern PyTorch compatibility\n",
            "Patched setup.py to return [] instead of None (avoids setup crash)\n"
          ]
        }
      ],
      "source": [
        "cuda_file = f\"/{HOME}/GroundingDINO/groundingdino/models/GroundingDINO/csrc/MsDeformAttn/ms_deform_attn_cuda.cu\"\n",
        "\n",
        "# Replace deprecated value.type() usage\n",
        "!sed -i '' 's/value\\.type()/value.scalar_type()/g' \"$cuda_file\"\n",
        "!sed -i '' 's/::detail::scalar_type(the_type)/the_type/g' \"$cuda_file\"\n",
        "!sed -i '' 's/value\\.scalar_type()\\.is_cuda()/value.is_cuda()/g' \"$cuda_file\"\n",
        "\n",
        "print(\"Fully patched CUDA file for modern PyTorch compatibility\")\n",
        "\n",
        "\n",
        "# === PATCH setup.py to return [] instead of None in get_extensions() ===\n",
        "setup_file = f\"/{HOME}/GroundingDINO/setup.py\"\n",
        "\n",
        "with open(setup_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "patched_lines = []\n",
        "inside_else_block = False\n",
        "\n",
        "for line in lines:\n",
        "    # Detect the \"else:\" block under the CUDA check\n",
        "    if line.strip() == \"else:\" and \"CUDA\" in \"\".join(patched_lines[-2:]):\n",
        "        inside_else_block = True\n",
        "        patched_lines.append(line)\n",
        "        continue\n",
        "\n",
        "    if inside_else_block:\n",
        "        if \"return None\" in line:\n",
        "            line = line.replace(\"return None\", \"return []  # patched for Colab\")\n",
        "            inside_else_block = False  # Only patch once\n",
        "    patched_lines.append(line)\n",
        "\n",
        "with open(setup_file, \"w\") as f:\n",
        "    f.writelines(patched_lines)\n",
        "\n",
        "print(\"Patched setup.py to return [] instead of None (avoids setup crash)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d_bdhDyQ8jr",
        "outputId": "cc81f91a-3454-4f25-c278-0652a7b623d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bertwarper.py patched – empty-TensorList crash eliminated\n"
          ]
        }
      ],
      "source": [
        "# ─── MICRO-PATCH GroundingDINO/bertwarper.py ──────────────────────────\n",
        "bert_file = f\"/{HOME}/GroundingDINO/groundingdino/models/GroundingDINO/bertwarper.py\"\n",
        "\n",
        "import re, pathlib, textwrap\n",
        "\n",
        "lines   = pathlib.Path(bert_file).read_text().splitlines(keepends=True)\n",
        "output  = []\n",
        "patched = False\n",
        "\n",
        "pat = re.compile(r\"^\\s*cate_to_token_mask_list\\s*=\\s*\\[\\s*torch\\.stack\")\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "\n",
        "    # ── find the original one-liner list-comprehension ────────────────\n",
        "    if not patched and pat.search(line):\n",
        "        indent = re.match(r\"^\\s*\", line).group(0)          # keep original indent\n",
        "\n",
        "        patch = textwrap.indent(textwrap.dedent(f\"\"\"\n",
        "            # ===== PATCHED: avoid empty TensorList crash =====\n",
        "            max_seq_len = input_ids.shape[1]\n",
        "            device      = input_ids.device\n",
        "            safe_cate_to_token_mask_list = []\n",
        "            for _mask_list in cate_to_token_mask_list:\n",
        "                if len(_mask_list) == 0:\n",
        "                    _mask_list = [torch.zeros((1, max_seq_len),\n",
        "                                             dtype=torch.bool,\n",
        "                                             device=device)]\n",
        "                safe_cate_to_token_mask_list.append(\n",
        "                    torch.stack(_mask_list, dim=0)\n",
        "                )\n",
        "            cate_to_token_mask_list = safe_cate_to_token_mask_list\n",
        "            # ===== END PATCH ===========================================\n",
        "        \"\"\"), indent)\n",
        "\n",
        "        output.append(patch)\n",
        "        patched = True\n",
        "        continue                      # ← skip the original risky line\n",
        "\n",
        "    # skip the second line of the old list-comprehension\n",
        "    if patched and \"for cate_to_token_mask_list\" in line:\n",
        "        continue\n",
        "\n",
        "    output.append(line)\n",
        "\n",
        "# write back\n",
        "pathlib.Path(bert_file).write_text(\"\".join(output))\n",
        "print(\"bertwarper.py patched – empty-TensorList crash eliminated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wtii3pK6-pd",
        "outputId": "54dfc8e3-bf06-4eaf-e7fc-c7a008ac83c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/GroundingDINO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (4.51.3)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (0.43.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (1.0.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (4.11.0.86)\n",
            "Requirement already satisfied: supervision>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (0.25.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (2.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (1.15.2)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm->groundingdino==0.1.0) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->groundingdino==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->groundingdino==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->groundingdino==0.1.0) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->groundingdino==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->groundingdino==0.1.0) (0.21.1)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->groundingdino==0.1.0) (4.3.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->groundingdino==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.17.0)\n",
            "Installing collected packages: groundingdino\n",
            "  Running setup.py develop for groundingdino\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -e . -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0NlacTF7PZb"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "print(f'HOME IS {HOME}')\n",
        "!wget -q -P weights https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GxzoAx69QuZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from timm import create_model\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.patches as patches\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Import GroundingDINO modules\n",
        "%cd {HOME}/GroundingDINO\n",
        "\n",
        "try:\n",
        "    from groundingdino.models import build_model\n",
        "    from groundingdino.util.slconfig import SLConfig\n",
        "    from groundingdino.util.utils import clean_state_dict\n",
        "    from groundingdino.util.inference import load_image, load_model, get_phrases_from_posmap\n",
        "    print(\"GroundingDINO modules imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing GroundingDINO modules: {e}\")\n",
        "    print(\"Please check your installation and try again.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "%cd {HOME}\n",
        "\n",
        "# Check if imports were successful\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TToUzhQL9QQM"
      },
      "source": [
        "# 2. RSVG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SFFTJnFCf4x"
      },
      "outputs": [],
      "source": [
        "class VisualEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisualEncoder, self).__init__()\n",
        "        self.backbone = create_model('resnet50', pretrained=True, features_only=True)\n",
        "        self.conv6_1 = nn.Conv2d(2048, 128, kernel_size=1, stride=1)\n",
        "        self.conv6_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv7_1 = nn.Conv2d(256, 128, kernel_size=1, stride=1)\n",
        "        self.conv7_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv8_1 = nn.Conv2d(256, 128, kernel_size=1, stride=1)\n",
        "        self.conv8_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        f1 = features[-1]\n",
        "        x = self.relu(self.conv6_1(f1))\n",
        "        f2 = self.relu(self.conv6_2(x))\n",
        "        x = self.relu(self.conv7_1(f2))\n",
        "        f3 = self.relu(self.conv7_2(x))\n",
        "        x = self.relu(self.conv8_1(f3))\n",
        "        f4 = self.relu(self.conv8_2(x))\n",
        "        return [f1, f2, f3, f4]\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def forward(self, texts):\n",
        "        if isinstance(texts, list) and isinstance(texts[0], str):\n",
        "            tokens = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=40)\n",
        "        else:\n",
        "            # Handle different text formats\n",
        "            if isinstance(texts, list) and isinstance(texts[0], dict) and \"caption\" in texts[0]:\n",
        "                text_strs = [text[\"caption\"] for text in texts]\n",
        "            else:\n",
        "                text_strs = [str(text) for text in texts]\n",
        "            tokens = self.tokenizer(text_strs, return_tensors='pt', padding=True, truncation=True, max_length=40)\n",
        "\n",
        "        device = next(self.bert.parameters()).device\n",
        "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "        outputs = self.bert(**tokens)\n",
        "        word_embeddings = outputs.last_hidden_state\n",
        "        sentence_embedding = outputs.pooler_output.unsqueeze(1)\n",
        "        return [word_embeddings, sentence_embedding]\n",
        "\n",
        "class MLCM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLCM, self).__init__()\n",
        "        self.linear_v1 = nn.Linear(2048, 256)\n",
        "        self.linear_v2 = nn.Linear(256, 256)\n",
        "        self.linear_v3 = nn.Linear(256, 256)\n",
        "        self.linear_v4 = nn.Linear(256, 256)\n",
        "        self.linear_t_word = nn.Linear(768, 256)\n",
        "        self.linear_t_sent = nn.Linear(768, 256)\n",
        "        self.L = 6\n",
        "        self.N = 6\n",
        "        self.cross_attn_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(256, 8, dropout=0.1) for _ in range(self.L)\n",
        "        ])\n",
        "        self.cross_attn_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(256) for _ in range(self.L)\n",
        "        ])\n",
        "        self.self_attn_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(256, 8, dropout=0.1) for _ in range(self.N)\n",
        "        ])\n",
        "        self.self_attn_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(256) for _ in range(self.N)\n",
        "        ])\n",
        "        self.ffn_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(256, 2048),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(2048, 256)\n",
        "            ) for _ in range(self.L + self.N)\n",
        "        ])\n",
        "        self.ffn_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(256) for _ in range(self.L + self.N)\n",
        "        ])\n",
        "\n",
        "    def forward(self, visual_features, text_features):\n",
        "        f1, f2, f3, f4 = visual_features\n",
        "        B, C, H, W = f1.shape\n",
        "        f1_flat = f1.reshape(B, C, H*W).permute(0, 2, 1)\n",
        "        f1_proj = self.linear_v1(f1_flat)\n",
        "        B, C, H, W = f2.shape\n",
        "        f2_flat = f2.reshape(B, C, H*W).permute(0, 2, 1)\n",
        "        f2_proj = self.linear_v2(f2_flat)\n",
        "        B, C, H, W = f3.shape\n",
        "        f3_flat = f3.reshape(B, C, H*W).permute(0, 2, 1)\n",
        "        f3_proj = self.linear_v3(f3_flat)\n",
        "        B, C, H, W = f4.shape\n",
        "        f4_flat = f4.reshape(B, C, H*W).permute(0, 2, 1)\n",
        "        f4_proj = self.linear_v4(f4_flat)\n",
        "        visual_proj = torch.cat([f1_proj, f2_proj, f3_proj, f4_proj], dim=1)\n",
        "        word_embeddings, sentence_embedding = text_features\n",
        "        word_proj = self.linear_t_word(word_embeddings)\n",
        "        sent_proj = self.linear_t_sent(sentence_embedding)\n",
        "        text_proj = torch.cat([word_proj, sent_proj], dim=1)\n",
        "        fvt = torch.cat([visual_proj, text_proj], dim=1)\n",
        "        x = f1_proj\n",
        "        for i in range(self.L):\n",
        "            x_trans = x.permute(1, 0, 2)\n",
        "            fvt_trans = fvt.permute(1, 0, 2)\n",
        "            attn_output, _ = self.cross_attn_layers[i](\n",
        "                query=x_trans,\n",
        "                key=fvt_trans,\n",
        "                value=fvt_trans\n",
        "            )\n",
        "            attn_output = attn_output.permute(1, 0, 2)\n",
        "            x = x + attn_output\n",
        "            x = self.cross_attn_norms[i](x)\n",
        "            ffn_output = self.ffn_layers[i](x)\n",
        "            x = x + ffn_output\n",
        "            x = self.ffn_norms[i](x)\n",
        "        for i in range(self.N):\n",
        "            x_trans = x.permute(1, 0, 2)\n",
        "            attn_output, _ = self.self_attn_layers[i](\n",
        "                query=x_trans,\n",
        "                key=x_trans,\n",
        "                value=x_trans\n",
        "            )\n",
        "            attn_output = attn_output.permute(1, 0, 2)\n",
        "            x = x + attn_output\n",
        "            x = self.self_attn_norms[i](x)\n",
        "            ffn_output = self.ffn_layers[i + self.L](x)\n",
        "            x = x + ffn_output\n",
        "            x = self.ffn_norms[i + self.L](x)\n",
        "        return x\n",
        "\n",
        "class MultimodalFusionModule(nn.Module):\n",
        "    def __init__(self, d_model=256):\n",
        "        super(MultimodalFusionModule, self).__init__()\n",
        "        self.learnable_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
        "        nn.init.normal_(self.learnable_token, std=0.02)\n",
        "        self.visual_proj = nn.Linear(256, d_model)\n",
        "        self.text_proj = nn.Linear(768, d_model)\n",
        "\n",
        "        # Set up transformer\n",
        "        transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=8,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1,\n",
        "            activation='relu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=6)\n",
        "\n",
        "    def forward(self, visual_features, text_features):\n",
        "        visual_tokens = self.visual_proj(visual_features)\n",
        "        word_embeddings = text_features[0]\n",
        "        text_tokens = self.text_proj(word_embeddings)\n",
        "        batch_size = visual_tokens.size(0)\n",
        "        learnable_token = self.learnable_token.expand(batch_size, -1, -1)\n",
        "        joint_tokens = torch.cat([learnable_token, visual_tokens, text_tokens], dim=1)\n",
        "        output = self.transformer(joint_tokens)\n",
        "        learnable_output = output[:, 0, :]\n",
        "        return learnable_output\n",
        "\n",
        "class LocalizationModule(nn.Module):\n",
        "    def __init__(self, d_model=256):\n",
        "        super(LocalizationModule, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class RSVGModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RSVGModel, self).__init__()\n",
        "        self.visual_encoder = VisualEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.mlcm = MLCM()\n",
        "        self.multimodal_fusion = MultimodalFusionModule()\n",
        "        self.localization = LocalizationModule()\n",
        "\n",
        "    def forward(self, images, texts):\n",
        "        visual_features = self.visual_encoder(images)\n",
        "        text_features = self.text_encoder(texts)\n",
        "        refined_visual_features = self.mlcm(visual_features, text_features)\n",
        "        fused_features = self.multimodal_fusion(refined_visual_features, text_features)\n",
        "        box_coords = self.localization(fused_features)\n",
        "        return box_coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWrbwd9E9SdA"
      },
      "source": [
        "# 3. LORA Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWRbBBlbCwv2"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Low-Rank Adaptation layer for efficient fine-tuning of pre-trained models\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, rank=4, alpha=16):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank\n",
        "\n",
        "        # LoRA weights\n",
        "        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))\n",
        "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Low-rank adaptation\n",
        "        return (x @ self.lora_A) @ self.lora_B * self.scaling\n",
        "\n",
        "def apply_lora_to_linear_layers(module, target_names=None, rank=4, alpha=16):\n",
        "    \"\"\"\n",
        "    Apply LoRA to linear layers in a model safely without changing dictionary size during iteration\n",
        "    \"\"\"\n",
        "    if target_names is None:\n",
        "        # Default to common attention projection layers\n",
        "        target_names = ['q_proj', 'k_proj', 'v_proj', 'out_proj', 'query', 'key', 'value']\n",
        "\n",
        "    lora_params = []\n",
        "\n",
        "    # First collect all the modules we want to modify\n",
        "    modules_to_modify = []\n",
        "    for name, submodule in module.named_modules():\n",
        "        if isinstance(submodule, nn.Linear) and any(target in name for target in target_names):\n",
        "            modules_to_modify.append((name, submodule))\n",
        "\n",
        "    # Then apply LoRA without modifying the dictionary during iteration\n",
        "    for name, submodule in modules_to_modify:\n",
        "        in_features, out_features = submodule.in_features, submodule.out_features\n",
        "\n",
        "        # Create a LoRA layer\n",
        "        lora_layer = LoRALayer(in_features, out_features, rank, alpha)\n",
        "\n",
        "        # Store original forward\n",
        "        original_forward = submodule.forward\n",
        "\n",
        "        # Create a new forward method that applies base + LoRA\n",
        "        def create_forward_hook(orig_forward, lora):\n",
        "            def forward_hook(x):\n",
        "                return orig_forward(x) + lora(x)\n",
        "            return forward_hook\n",
        "\n",
        "        # Set the new forward method\n",
        "        submodule.forward = create_forward_hook(original_forward, lora_layer)\n",
        "\n",
        "        # Store the lora_layer as a direct attribute of the parent module\n",
        "        # Use a sanitized name to avoid issues with dots in attribute names\n",
        "        lora_name = f\"{name.replace('.', '_')}_lora\"\n",
        "        setattr(module, lora_name, lora_layer)\n",
        "\n",
        "        # Add parameters to the list of trainable parameters\n",
        "        lora_params.extend(list(lora_layer.parameters()))\n",
        "\n",
        "        print(f\"Applied LoRA to {name}\")\n",
        "\n",
        "    return lora_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jlMc8T89UpX"
      },
      "source": [
        "# 4. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRVwGFSLKyyd"
      },
      "outputs": [],
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETjwUh2I7SWr"
      },
      "outputs": [],
      "source": [
        "class RSVGLoss(nn.Module):\n",
        "    def __init__(self, lambda_giou=1.0):\n",
        "        super(RSVGLoss, self).__init__()\n",
        "        self.smooth_l1 = nn.SmoothL1Loss()\n",
        "        self.lambda_giou = lambda_giou\n",
        "\n",
        "    def forward(self, pred_boxes, target_boxes):\n",
        "        smooth_l1_loss = self.smooth_l1(pred_boxes, target_boxes)\n",
        "        giou_loss = self.generalized_box_iou_loss(pred_boxes, target_boxes)\n",
        "        total_loss = smooth_l1_loss + self.lambda_giou * giou_loss\n",
        "        return total_loss\n",
        "\n",
        "    def generalized_box_iou_loss(self, pred_boxes, target_boxes):\n",
        "        pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
        "        target_area = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n",
        "        left_top = torch.max(pred_boxes[:, :2], target_boxes[:, :2])\n",
        "        right_bottom = torch.min(pred_boxes[:, 2:], target_boxes[:, 2:])\n",
        "        wh = (right_bottom - left_top).clamp(min=0)\n",
        "        intersection = wh[:, 0] * wh[:, 1]\n",
        "        union = pred_area + target_area - intersection\n",
        "        iou = intersection / (union + 1e-7)\n",
        "        enclosing_left_top = torch.min(pred_boxes[:, :2], target_boxes[:, :2])\n",
        "        enclosing_right_bottom = torch.max(pred_boxes[:, 2:], target_boxes[:, 2:])\n",
        "        enclosing_wh = (enclosing_right_bottom - enclosing_left_top).clamp(min=0)\n",
        "        enclosing_area = enclosing_wh[:, 0] * enclosing_wh[:, 1]\n",
        "        giou = iou - (enclosing_area - union) / (enclosing_area + 1e-7)\n",
        "        return 1 - giou.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUz3lF6HDAGe"
      },
      "outputs": [],
      "source": [
        "class RemoteSensingDataset(Dataset):\n",
        "    def __init__(self, img_dir, ann_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.ann_dir = ann_dir\n",
        "        self.imgs = [f for f in os.listdir(self.img_dir) if f.endswith(('.jpg', '.png'))]\n",
        "        if transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((640, 640)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.imgs[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        xml_path = os.path.join(self.ann_dir, img_name.replace('.jpg', '.xml').replace('.png', '.xml'))\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            w, h = image.size\n",
        "            image = self.transform(image)\n",
        "\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "            obj = root.find('object')\n",
        "            if obj is None:\n",
        "                return None\n",
        "\n",
        "            grounding_caption_elem = root.find('grounding_caption')\n",
        "            if grounding_caption_elem is not None and grounding_caption_elem.text is not None:\n",
        "                query = grounding_caption_elem.text.strip()\n",
        "                print(f'CAPTION NOT FOUND')\n",
        "            else:\n",
        "                description = obj.find('description')\n",
        "                if description is None or description.text is None:\n",
        "                    query = \"object\"\n",
        "                else:\n",
        "                    query = description.text.strip()\n",
        "\n",
        "            bbox = obj.find('bndbox')\n",
        "            if bbox is None:\n",
        "                return None\n",
        "\n",
        "            x1 = float(bbox.find('xmin').text)\n",
        "            y1 = float(bbox.find('ymin').text)\n",
        "            x2 = float(bbox.find('xmax').text)\n",
        "            y2 = float(bbox.find('ymax').text)\n",
        "\n",
        "            # Normalize coordinates\n",
        "            box = torch.tensor([x1/w, y1/h, x2/w, y2/h], dtype=torch.float32)\n",
        "\n",
        "            return image, query, box\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {img_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "def compute_iou(pred_box, true_box):\n",
        "    \"\"\"\n",
        "    Compute IoU between predicted and ground truth boxes\n",
        "    Boxes are in format [x1, y1, x2, y2]\n",
        "    \"\"\"\n",
        "    # Ensure tensors\n",
        "    if not isinstance(pred_box, torch.Tensor):\n",
        "        pred_box = torch.tensor(pred_box)\n",
        "    if not isinstance(true_box, torch.Tensor):\n",
        "        true_box = torch.tensor(true_box)\n",
        "\n",
        "    if pred_box.nelement() == 0 or true_box.nelement() == 0:\n",
        "        return 0.0\n",
        "    # Calculate intersection\n",
        "    x1_inter = torch.max(pred_box[0], true_box[0])\n",
        "    y1_inter = torch.max(pred_box[1], true_box[1])\n",
        "    x2_inter = torch.min(pred_box[2], true_box[2])\n",
        "    y2_inter = torch.min(pred_box[3], true_box[3])\n",
        "\n",
        "    width_inter = torch.max(torch.tensor(0.0), x2_inter - x1_inter)\n",
        "    height_inter = torch.max(torch.tensor(0.0), y2_inter - y1_inter)\n",
        "    area_inter = width_inter * height_inter\n",
        "\n",
        "    # Calculate areas\n",
        "    area_pred = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
        "    area_true = (true_box[2] - true_box[0]) * (true_box[3] - true_box[1])\n",
        "\n",
        "    # Calculate union\n",
        "    area_union = area_pred + area_true - area_inter\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = area_inter / (area_union + 1e-7)\n",
        "\n",
        "    return iou\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Remove None samples\n",
        "    batch = [x for x in batch if x is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "    images, queries, boxes = zip(*batch)\n",
        "    images = torch.stack(images, dim=0)\n",
        "    boxes = torch.stack(boxes, dim=0)\n",
        "    return images, list(queries), boxes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzSxFoMU9XWN"
      },
      "source": [
        "# 5. RSVG_DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEZQISKVDBMr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.misc import clean_state_dict\n",
        "\n",
        "class RSVG_DINO(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        rsvg_model_path: str,\n",
        "        config_path: str,\n",
        "        groundingdino_weights_path: str\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # RSVG model\n",
        "        print(\"Loading RSVG model...\")\n",
        "        self.rsvg = RSVGModel()\n",
        "        ckpt = torch.load(rsvg_model_path, map_location='cpu')\n",
        "        rsvg_state = ckpt.get(\"model_state_dict\", ckpt)\n",
        "        self.rsvg.load_state_dict(rsvg_state, strict=False)\n",
        "        self.rsvg.eval()\n",
        "        for p in self.rsvg.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        print(\"RSVG model loaded successfully!\")\n",
        "\n",
        "        # GroundingDINO model\n",
        "        print(\"Loading GroundingDINO model...\")\n",
        "        args = SLConfig.fromfile(config_path)\n",
        "        self.groundingdino = build_model(args)\n",
        "        dino_ckpt = torch.load(groundingdino_weights_path, map_location='cpu')\n",
        "        dino_state = clean_state_dict(dino_ckpt.get(\"model\", dino_ckpt))\n",
        "        self.groundingdino.load_state_dict(dino_state, strict=False)\n",
        "        self.groundingdino.eval()\n",
        "        print(\"GroundingDINO model loaded successfully!\")\n",
        "\n",
        "        # Default fallback for tokens\n",
        "        self.default_tokens_positive = [[1]]\n",
        "\n",
        "        # Projection & fusion heads\n",
        "        self.dino_proj = nn.Linear(256, 256)\n",
        "        self.feature_fusion = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.confidence_weighting = nn.Linear(256, 2)\n",
        "        self.enhanced_localization = nn.Linear(256, 4)\n",
        "\n",
        "        # Initialize weights to favor RSVG initially\n",
        "        with torch.no_grad():\n",
        "            if hasattr(self.confidence_weighting, 'bias'):\n",
        "                self.confidence_weighting.bias.data[0] = 1.0  # Higher bias for RSVG\n",
        "                self.confidence_weighting.bias.data[1] = 0.0  # Lower bias for DINO\n",
        "\n",
        "        print(\"RSVG_DINO initialization complete!\")\n",
        "\n",
        "        # Debug mode\n",
        "        self.debug = True\n",
        "\n",
        "    def forward(self, images, texts):\n",
        "        B = images.shape[0]\n",
        "        device = images.device\n",
        "\n",
        "        if self.debug:\n",
        "            print(\"\\n==== DEBUG: RSVG_DINO.forward ====\")\n",
        "            print(f\"Batch size: {B}\")\n",
        "            print(f\"Images shape: {images.shape}\")\n",
        "            print(f\"Texts type: {type(texts)}, length: {len(texts)}\")\n",
        "            for i, t in enumerate(texts[:min(3, len(texts))]):  # Print first few\n",
        "               print(f\"  Text {i}: {t}\")\n",
        "            if len(texts) > 3:\n",
        "                print(f\"  ... and {len(texts) - 3} more\")\n",
        "\n",
        "           # Send models to device\n",
        "        self.rsvg.to(device)\n",
        "        self.groundingdino.to(device)\n",
        "\n",
        "        # RSVG\n",
        "        if self.debug:\n",
        "            print(\"\\n--- Processing RSVG pipeline ---\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get inputs for RSVG\n",
        "            if isinstance(texts[0], dict) and \"caption\" in texts[0]:\n",
        "                rsvg_texts = [t[\"caption\"] for t in texts]\n",
        "            else:\n",
        "                rsvg_texts = texts\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"RSVG texts: {rsvg_texts[:min(3, len(rsvg_texts))]}\")\n",
        "\n",
        "            # Process through RSVG pipeline\n",
        "            vf = self.rsvg.visual_encoder(images)\n",
        "            tf = self.rsvg.text_encoder(rsvg_texts)\n",
        "            rv = self.rsvg.mlcm(vf, tf)\n",
        "            rsvg_feats = self.rsvg.multimodal_fusion(rv, tf)\n",
        "            rsvg_boxes = self.rsvg.localization(rsvg_feats)\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"RSVG features shape: {rsvg_feats.shape}\")\n",
        "                print(f\"RSVG boxes shape: {rsvg_boxes.shape}\")\n",
        "                print(f\"RSVG sample boxes: {rsvg_boxes[0]}\")\n",
        "\n",
        "        # GROUNDING DINO\n",
        "        if self.debug:\n",
        "            print(\"\\n--- Processing GroundingDINO pipeline ---\")\n",
        "\n",
        "        try:\n",
        "            # Format queries with guaranteed valid token spans\n",
        "            # from transformers import BertTokenizerFast\n",
        "            tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "            dino_texts = []\n",
        "            for cap in texts:\n",
        "                # If input is dict (from custom collate), extract \"caption\"\n",
        "                if isinstance(cap, dict) and \"caption\" in cap:\n",
        "                    caption = cap[\"caption\"]\n",
        "                else:\n",
        "                    caption = cap\n",
        "                ids = tokenizer(caption, add_special_tokens=True).input_ids\n",
        "                seq_len = len(ids)\n",
        "                tokens_pos = list(range(1, seq_len - 1))\n",
        "                if not tokens_pos:\n",
        "                    tokens_pos = [1]\n",
        "                dino_texts.append({\n",
        "                    \"caption\": caption,\n",
        "                    \"tokens_positive\": [tokens_pos],\n",
        "                })\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"Formatted {len(dino_texts)} texts for GroundingDINO\")\n",
        "                print(f\"Sample formatted text: {dino_texts[0]}\")\n",
        "\n",
        "            # IMPORTANT: Add special attribute to help bertwarper\n",
        "            self.groundingdino.specical_tokens = [q[\"tokens_positive\"][0] for q in dino_texts]\n",
        "\n",
        "            if self.debug:\n",
        "                print(\"Set self.groundingdino.specical_tokens to:\")\n",
        "                for i, tok in enumerate(self.groundingdino.specical_tokens[:min(3, len(self.groundingdino.specical_tokens))]):\n",
        "                    print(f\"  Sample {i}: {tok}\")\n",
        "\n",
        "            # CRITICAL DEBUGGING POINT - Before actual GroundingDINO forward\n",
        "            if self.debug:\n",
        "                print(\"\\n>>> About to call groundingdino forward <<<\")\n",
        "                print(f\"groundingdino input image shape: {images.shape}\")\n",
        "                print(f\"groundingdino input texts: {len(dino_texts)} items\")\n",
        "                try:\n",
        "                    model_state = {\n",
        "                        \"has_tokenizer\": hasattr(self.groundingdino, \"tokenizer\"),\n",
        "                        \"has_special_tokens\": hasattr(self.groundingdino, \"specical_tokens\"),\n",
        "                        \"tokenizer_type\": type(self.groundingdino.tokenizer).__name__ if hasattr(self.groundingdino, \"tokenizer\") else None,\n",
        "                    }\n",
        "                    print(f\"Model state checks: {model_state}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during model inspection: {e}\")\n",
        "\n",
        "            # Process through GroundingDINO\n",
        "            dino_out = self.groundingdino(images, dino_texts)\n",
        "\n",
        "            if self.debug:\n",
        "                print(\"\\n>>> GroundingDINO forward succeeded! <<<\")\n",
        "                print(f\"Output keys: {list(dino_out.keys())}\")\n",
        "\n",
        "            # Extract outputs\n",
        "            logits = dino_out[\"pred_logits\"]\n",
        "            boxes = dino_out[\"pred_boxes\"]\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"Pred logits shape: {logits.shape}\")\n",
        "                print(f\"Pred boxes shape: {boxes.shape}\")\n",
        "\n",
        "            # Get feature embeddings\n",
        "            if \"hidden_states\" in dino_out:\n",
        "                df = dino_out[\"hidden_states\"][-1][:, 0, :]\n",
        "                if self.debug:\n",
        "                    print(f\"Using hidden_states for features, shape: {df.shape}\")\n",
        "            elif \"encoder_hidden_states\" in dino_out:\n",
        "                df = dino_out[\"encoder_hidden_states\"][-1][:, 0, :]\n",
        "                if self.debug:\n",
        "                    print(f\"Using encoder_hidden_states for features, shape: {df.shape}\")\n",
        "            else:\n",
        "                df = torch.zeros(B, 256, device=device)\n",
        "                if self.debug:\n",
        "                    print(\"No hidden states found, using zeros\")\n",
        "\n",
        "            # Process boxes and scores\n",
        "            box_list, score_list = [], []\n",
        "            for b in range(B):\n",
        "                scores = F.softmax(logits[b, :, 0], dim=0)\n",
        "                if self.debug and b == 0:\n",
        "                    print(f\"Sample scores shape: {scores.shape}\")\n",
        "                    print(f\"Sample scores values: {scores[:5]}\")  # First 5 scores\n",
        "\n",
        "                if scores.numel() > 0:\n",
        "                    # Get best prediction\n",
        "                    i = scores.argmax()\n",
        "                    s = scores[i]\n",
        "                    c = boxes[b, i]\n",
        "\n",
        "                    # Convert from center-size to corners\n",
        "                    x1 = c[0] - c[2] / 2\n",
        "                    y1 = c[1] - c[3] / 2\n",
        "                    x2 = c[0] + c[2] / 2\n",
        "                    y2 = c[1] + c[3] / 2\n",
        "\n",
        "                    box_list.append(torch.stack([x1, y1, x2, y2], dim=0))\n",
        "                    score_list.append(s)\n",
        "\n",
        "                    if self.debug and b == 0:\n",
        "                        print(f\"Best box for sample 0: {[x1.item(), y1.item(), x2.item(), y2.item()]}\")\n",
        "                        print(f\"Confidence score: {s.item()}\")\n",
        "                else:\n",
        "                    # Default box (full image) with zero confidence\n",
        "                    box_list.append(torch.tensor([0, 0, 1, 1], device=device))\n",
        "                    score_list.append(torch.tensor(0.0, device=device))\n",
        "\n",
        "                    if self.debug and b == 0:\n",
        "                        print(\"Using default box [0,0,1,1] with zero confidence\")\n",
        "\n",
        "            # Stack results\n",
        "            dino_boxes = torch.stack(box_list)\n",
        "            dino_scores = torch.stack(score_list)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug:\n",
        "                print(f\"\\nERROR in GroundingDINO processing: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "            # Use default values on error\n",
        "            df = torch.zeros(B, 256, device=device)\n",
        "            dino_boxes = torch.zeros(B, 4, device=device)\n",
        "            dino_scores = torch.zeros(B, device=device)\n",
        "\n",
        "        # FEATURE FUSION & OUTPUT\n",
        "        if self.debug:\n",
        "            print(\"\\n--- Processing Feature Fusion ---\")\n",
        "\n",
        "        df_proj = self.dino_proj(df)\n",
        "        fused_in = torch.cat([rsvg_feats, df_proj], dim=1)\n",
        "        fused_feats = self.feature_fusion(fused_in)\n",
        "        weights = F.softmax(self.confidence_weighting(fused_feats), dim=1)\n",
        "        enhanced_bxes = self.enhanced_localization(fused_feats)\n",
        "\n",
        "        # Weighted average of boxes based on confidence\n",
        "        weighted_bxes = weights[:, 0:1] * rsvg_boxes + weights[:, 1:2] * dino_boxes\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Fusion weights: RSVG={weights[0, 0].item():.4f}, DINO={weights[0, 1].item():.4f}\")\n",
        "            print(f\"Final boxes sample: {weighted_bxes[0]}\")\n",
        "            print(\"==== END DEBUG: RSVG_DINO.forward ====\\n\")\n",
        "\n",
        "        return {\n",
        "            \"boxes\": weighted_bxes,\n",
        "            \"enhanced_boxes\": enhanced_bxes,\n",
        "            \"rsvg_boxes\": rsvg_boxes,\n",
        "            \"dino_boxes\": dino_boxes,\n",
        "            \"dino_scores\": dino_scores,\n",
        "            \"confidence_weights\": weights,\n",
        "            \"fused_features\": fused_feats\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3244V_aA9ao8"
      },
      "source": [
        "# 6. Training Loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt-6NpA-DXQt"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# def train_rsvgdino_model(\n",
        "#     model,\n",
        "#     train_loader,\n",
        "#     criterion,\n",
        "#     optimizer,\n",
        "#     device,\n",
        "#     checkpoint_dir,\n",
        "#     num_epochs\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     model         : your RSVG_DINO\n",
        "#     train_loader  : DataLoader yielding (images, texts, targets)\n",
        "#     criterion     : RSVGLoss(pred_boxes, target_boxes)\n",
        "#     optimizer     : as you already built it\n",
        "#     device        : torch.device\n",
        "#     checkpoint_dir: path to save checkpoints\n",
        "#     num_epochs    : int\n",
        "#     \"\"\"\n",
        "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "#     history = {\"loss\": []}\n",
        "\n",
        "#     for epoch in range(1, num_epochs + 1):\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "\n",
        "#         loop = tqdm(train_loader, desc=f\"[Epoch {epoch}/{num_epochs}]\", leave=False)\n",
        "#         for batch_idx, (images, texts, targets) in enumerate(loop, start=1):\n",
        "#             # ---- 1) move images to device ----\n",
        "#             images = images.to(device)\n",
        "\n",
        "#             # ---- 2) pull out target_boxes ----\n",
        "#             if isinstance(targets, dict) and \"boxes\" in targets:\n",
        "#                 # your collate returned a dict of batched tensors\n",
        "#                 target_boxes = targets[\"boxes\"].to(device)\n",
        "\n",
        "#             elif torch.is_tensor(targets):\n",
        "#                 # collate returned a single tensor of shape [B,4]\n",
        "#                 target_boxes = targets.to(device)\n",
        "\n",
        "#             elif isinstance(targets, (list, tuple)) and torch.is_tensor(targets[0]):\n",
        "#                 # collate returned list of per-sample box tensors [4] -> stack into [B,4]\n",
        "#                 target_boxes = torch.stack([t.to(device) for t in targets], dim=0)\n",
        "\n",
        "#             else:\n",
        "#                 raise ValueError(f\"Unrecognized targets format: {type(targets)}\")\n",
        "\n",
        "#             # ---- 3) forward + loss ----\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(images, texts)\n",
        "#             pred_boxes = outputs[\"boxes\"]\n",
        "\n",
        "#             loss = criterion(pred_boxes, target_boxes)\n",
        "\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # ---- 4) logging ----\n",
        "#             running_loss += loss.item()\n",
        "#             if batch_idx % 10 == 0:\n",
        "#                 avg_batch_loss = running_loss / batch_idx\n",
        "#                 print(f\"[Epoch {epoch}] Batch {batch_idx}/{len(train_loader)} — avg loss: {avg_batch_loss:.4f}\")\n",
        "#             loop.set_postfix(loss=running_loss / batch_idx)\n",
        "\n",
        "#         # end of epoch\n",
        "#         avg_loss = running_loss / len(train_loader)\n",
        "#         history[\"loss\"].append(avg_loss)\n",
        "#         print(f\"Epoch {epoch}/{num_epochs} — avg loss: {avg_loss:.4f}\")\n",
        "\n",
        "#         # save every 5 epochs\n",
        "#         if epoch % 5 == 0:\n",
        "#             ckpt_name = f\"rsvgdino_{epoch}.pth\"\n",
        "#             ckpt_path = os.path.join(checkpoint_dir, ckpt_name)\n",
        "#             torch.save(model.state_dict(), ckpt_path)\n",
        "#             print(f\"Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "#     return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IbHnX-dwDfNW",
        "outputId": "4a33c43c-e42f-4503-8424-ad7a6c5d212d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Training dataset contains 12340 samples\n",
            "Loading RSVG model...\n",
            "RSVG model loaded successfully!\n",
            "Loading GroundingDINO model...\n",
            "final text_encoder_type: bert-base-uncased\n",
            "GroundingDINO model loaded successfully!\n",
            "RSVG_DINO initialization complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "# 1. Configuration\n",
        "batch_size = 14\n",
        "num_epochs = 20\n",
        "\n",
        "# 2. paths to pretrained RSVG & DINO\n",
        "rsvg_model_path            = f\"/{HOME}/drive/MyDrive/Checkpoints/rsvg_checkpoint_epoch_150.pth\"\n",
        "config_path                = f\"/{HOME}/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "groundingdino_weights_path = f\"/{HOME}/weights/groundingdino_swint_ogc.pth\"\n",
        "\n",
        "# 3. where to save your new checkpoints\n",
        "checkpoint_dir = f\"{HOME}/drive/MyDrive/Checkpoints2\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# 3. Data directories\n",
        "train_img_dir = f\"{HOME}/drive/MyDrive/Dataset/train_data/train_images\"\n",
        "train_ann_dir = f\"{HOME}/drive/MyDrive/Dataset/train_data/train_annotations\"\n",
        "\n",
        "# 5. Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 6. Dataset & DataLoader\n",
        "train_dataset = RemoteSensingDataset(\n",
        "    img_dir=train_img_dir,\n",
        "    ann_dir=train_ann_dir\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=custom_collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Training dataset contains {len(train_dataset)} samples\")\n",
        "\n",
        "# 7. Model\n",
        "model = RSVG_DINO(\n",
        "    rsvg_model_path=rsvg_model_path,\n",
        "    config_path=config_path,\n",
        "    groundingdino_weights_path=groundingdino_weights_path\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# 8 Optimizer\n",
        "backbone_params = []\n",
        "text_params     = []\n",
        "fusion_params   = []\n",
        "lora_params     = []\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if not param.requires_grad:\n",
        "        continue\n",
        "\n",
        "    # LoRA adapters\n",
        "    if \"lora\" in name:\n",
        "        lora_params.append(param)\n",
        "\n",
        "    # Vision & DINO backbones\n",
        "    elif name.startswith(\"rsvg.visual_encoder.backbone\") \\\n",
        "      or name.startswith(\"groundingdino.backbone\") \\\n",
        "      or name.startswith(\"groundingdino.input_proj\"):\n",
        "        backbone_params.append(param)\n",
        "\n",
        "    # Text encoder (BERT)\n",
        "    elif name.startswith(\"rsvg.text_encoder.bert\") \\\n",
        "      or name.startswith(\"groundingdino.bert\"):\n",
        "        text_params.append(param)\n",
        "\n",
        "    # Everything else new: fusion, heads, MLCM, projections\n",
        "    else:\n",
        "        fusion_params.append(param)\n",
        "\n",
        "optimizer = AdamW([\n",
        "    { \"params\": backbone_params, \"lr\": 5e-6 },   # backbone\n",
        "    { \"params\": text_params,     \"lr\": 1e-5 },   # text\n",
        "    { \"params\": fusion_params,   \"lr\": 2e-5 },   # fusion heads\n",
        "    { \"params\": lora_params,     \"lr\": 8e-5 },   # LoRA\n",
        "], weight_decay=1e-2)\n",
        "\n",
        "\n",
        "# 9. Loss function\n",
        "criterion = RSVGLoss(lambda_giou=1.0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gmptB0_y7SWt"
      },
      "outputs": [],
      "source": [
        "# # Assume you have already loaded your model as `model`\n",
        "# # and have a batch of images and queries (texts)\n",
        "\n",
        "# # Example: get a batch from your DataLoader\n",
        "# batch = next(iter(train_loader))  # or test_loader\n",
        "# images, queries, boxes = batch  # boxes are optional for inference\n",
        "\n",
        "# # Move images to device\n",
        "# images = images.to(device)\n",
        "\n",
        "# # Call the model directly\n",
        "# outputs = model(images, queries)\n",
        "\n",
        "# # outputs is a dict with keys: \"boxes\", \"enhanced_boxes\", etc.\n",
        "# print(f' BOXES: {outputs[\"boxes\"]}')\n",
        "# print(f' ENHANCED BOXES: {outputs[\"enhanced_boxes\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TTkW4G79a3uq"
      },
      "outputs": [],
      "source": [
        "# # Kick off training\n",
        "# trained_model, history = train_rsvgdino_model(\n",
        "#     model=model,\n",
        "#     train_loader=train_loader,\n",
        "#     criterion=criterion,\n",
        "#     optimizer=optimizer,\n",
        "#     device=device,\n",
        "#     checkpoint_dir=checkpoint_dir,\n",
        "#     num_epochs=num_epochs\n",
        "# )\n",
        "\n",
        "# print(\"Training completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UKx5okQe9oto",
        "outputId": "b2cf4d90-1c48-4ab5-97a5-74f816503dbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v08mnlNXqXS5",
        "outputId": "0380dec9-2687-44a3-d5be-2d747e387145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install -q opencv-python-headless tqdm\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def load_sam_predictor(sam_checkpoint_path, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Loads SAM ViT-B model as a box-refinement predictor.\n",
        "    \"\"\"\n",
        "    sam = sam_model_registry[\"vit_b\"](checkpoint=sam_checkpoint_path).to(device).eval()\n",
        "    return SamPredictor(sam)\n",
        "\n",
        "def refine_boxes_with_sam(image_np, boxes_np, predictor):\n",
        "    \"\"\"\n",
        "    Refines each predicted box using SAM to snap it to object boundaries.\n",
        "\n",
        "    Args:\n",
        "        image_np: (H, W, 3) RGB image as numpy array (uint8)\n",
        "        boxes_np: (N, 4) array of predicted boxes (xyxy)\n",
        "        predictor: SamPredictor instance\n",
        "\n",
        "    Returns:\n",
        "        (N, 4) array of tightened boxes\n",
        "    \"\"\"\n",
        "    refined_boxes = []\n",
        "    predictor.set_image(image_np)\n",
        "\n",
        "    for box in boxes_np:\n",
        "        box_tensor = torch.tensor(box, dtype=torch.float32).reshape(1, 4)\n",
        "        masks, _, _ = predictor.predict(box=box_tensor.numpy(), multimask_output=False)\n",
        "        mask = masks[0]\n",
        "\n",
        "        ys, xs = np.where(mask)\n",
        "        if len(xs) == 0 or len(ys) == 0:\n",
        "            refined_boxes.append(box)\n",
        "            continue\n",
        "\n",
        "        x0, y0, x1, y1 = xs.min(), ys.min(), xs.max(), ys.max()\n",
        "        refined_boxes.append([x0, y0, x1, y1])\n",
        "\n",
        "    return np.array(refined_boxes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_zODn4ch77dN"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_with_sam(model, dataloader, sam_checkpoint_path, device=\"cuda\", iou_threshold=0.5):\n",
        "    model.eval()\n",
        "    predictor = load_sam_predictor(sam_checkpoint_path, device)\n",
        "\n",
        "    correct, total_preds, total_gts = 0, 0, 0\n",
        "    all_ious = []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        images, texts, gt_boxes = batch  # from your dataloader\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            img_tensor = images[i].unsqueeze(0).to(device)  # (1, C, H, W)\n",
        "            text = [texts[i]]  # wrap single text into list\n",
        "            gt = gt_boxes[i].cpu().numpy()\n",
        "\n",
        "            # Run forward pass\n",
        "            with torch.no_grad():\n",
        "                out = model(img_tensor, text)\n",
        "\n",
        "                # Change pred_boxes to remove the squeeze() or handle the case where it contains only one element\n",
        "                # Original Line: pred_boxes = out[\"boxes\"].squeeze(0).cpu().numpy()\n",
        "                pred_boxes = out[\"boxes\"].cpu().numpy()\n",
        "                if pred_boxes.ndim == 1:\n",
        "                    pred_boxes = pred_boxes[np.newaxis, :]  # add a dimension if it's a 1D array\n",
        "\n",
        "            # Prepare image for SAM\n",
        "            image_np = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "            image_np = ((image_np * 255).clip(0, 255)).astype(np.uint8)\n",
        "\n",
        "            # SAM refinement\n",
        "            refined = refine_boxes_with_sam(image_np, pred_boxes, predictor)\n",
        "\n",
        "            matched = set()\n",
        "            for p in refined:\n",
        "                for j, g in enumerate(gt):\n",
        "                    iou = compute_iou(p, g)\n",
        "                    if iou >= iou_threshold and j not in matched:\n",
        "                        correct += 1\n",
        "                        matched.add(j)\n",
        "                        all_ious.append(iou)\n",
        "                        break\n",
        "\n",
        "            total_preds += len(refined)\n",
        "            total_gts += len(gt)\n",
        "\n",
        "    precision = correct / total_preds if total_preds > 0 else 0\n",
        "    recall = correct / total_gts if total_gts > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "    mean_iou = np.mean(all_ious) if all_ious else 0\n",
        "\n",
        "    print(f\"📊 Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f} | mIoU: {mean_iou:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-Cf9C8hrA_xy",
        "outputId": "88e76108-0797-4849-a458-11a00cd28ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test dataset contains 3372 samples.\n"
          ]
        }
      ],
      "source": [
        "# Test dataset paths\n",
        "test_img_dir = \"/content/drive/MyDrive/Dataset/test_data/test_images\"\n",
        "test_ann_dir = \"/content/drive/MyDrive/Dataset/test_data/test_annotations\"\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = RemoteSensingDataset(\n",
        "    img_dir=test_img_dir,\n",
        "    ann_dir=test_ann_dir\n",
        ")\n",
        "\n",
        "# Create test DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,  # for SAM accuracy, 1 image at a time is best\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=custom_collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Test dataset contains {len(test_dataset)} samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "09c0c03bf0044f0397019ca84b02876b"
          ]
        },
        "id": "y_w1Jvnq8CZe",
        "outputId": "f8e478a8-66e4-4473-eac4-672a15539aac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09c0c03bf0044f0397019ca84b02876b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/3372 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== DEBUG: RSVG_DINO.forward ====\n",
            "Batch size: 1\n",
            "Images shape: torch.Size([1, 3, 640, 640])\n",
            "Texts type: <class 'list'>, length: 1\n",
            "  Text 0: The golf field at the bottom\n",
            "\n",
            "--- Processing RSVG pipeline ---\n",
            "RSVG texts: ['The golf field at the bottom']\n",
            "RSVG features shape: torch.Size([1, 256])\n",
            "RSVG boxes shape: torch.Size([1, 4])\n",
            "RSVG sample boxes: tensor([0.2451, 0.4868, 0.7370, 0.9557], device='cuda:0')\n",
            "\n",
            "--- Processing GroundingDINO pipeline ---\n",
            "Formatted 1 texts for GroundingDINO\n",
            "Sample formatted text: {'caption': 'The golf field at the bottom', 'tokens_positive': [[1, 2, 3, 4, 5, 6]]}\n",
            "Set self.groundingdino.specical_tokens to:\n",
            "  Sample 0: [1, 2, 3, 4, 5, 6]\n",
            "\n",
            ">>> About to call groundingdino forward <<<\n",
            "groundingdino input image shape: torch.Size([1, 3, 640, 640])\n",
            "groundingdino input texts: 1 items\n",
            "Model state checks: {'has_tokenizer': True, 'has_special_tokens': True, 'tokenizer_type': 'BertTokenizerFast'}\n",
            "\n",
            "ERROR in GroundingDINO processing: stack expects a non-empty TensorList\n",
            "\n",
            "--- Processing Feature Fusion ---\n",
            "Fusion weights: RSVG=0.7261, DINO=0.2739\n",
            "Final boxes sample: tensor([0.1780, 0.3534, 0.5351, 0.6939], device='cuda:0')\n",
            "==== END DEBUG: RSVG_DINO.forward ====\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-104-d52a0d23cdf8>\", line 160, in forward\n",
            "    dino_out = self.groundingdino(images, dino_texts)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py\", line 255, in forward\n",
            "    ) = generate_masks_with_special_tokens_and_transfer_map(\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/GroundingDINO/groundingdino/models/GroundingDINO/bertwarper.py\", line 264, in generate_masks_with_special_tokens_and_transfer_map\n",
            "    cate_to_token_mask_list = [\n",
            "                              ^\n",
            "  File \"/content/GroundingDINO/groundingdino/models/GroundingDINO/bertwarper.py\", line 265, in <listcomp>\n",
            "    torch.stack(cate_to_token_mask_listi, dim=0)\n",
            "RuntimeError: stack expects a non-empty TensorList\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-dc944844cd6e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msam_ckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Checkpoints2/sam_vit_b_01ec64.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mevaluate_model_with_sam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msam_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-111-4c67de5c4a76>\u001b[0m in \u001b[0;36mevaluate_model_with_sam\u001b[0;34m(model, dataloader, sam_checkpoint_path, device, iou_threshold)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrefined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0miou\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0miou_threshold\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-103-cbae0476feac>\u001b[0m in \u001b[0;36mcompute_iou\u001b[0;34m(pred_box, true_box)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Calculate intersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mx1_inter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0my1_inter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mx2_inter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        }
      ],
      "source": [
        "sam_ckpt = \"/content/drive/MyDrive/Checkpoints2/sam_vit_b_01ec64.pth\"\n",
        "\n",
        "evaluate_model_with_sam(model, test_loader, sam_ckpt)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
